{
  "cells": [
    {
      "metadata": {
        "id": "ehS5McPJTqO7"
      },
      "cell_type": "markdown",
      "source": [
        "### Train for MICCAI challenge on colab using data on gDrive"
      ]
    },
    {
      "metadata": {
        "id": "v92Q3jB2TqO8",
        "outputId": "690506cd-531b-48de-faf7-d61085582442",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (91.189.91.83)] [1 InReleas\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (91.189.91.83)] [Connected \r                                                                                                    \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Ign:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,173 kB]\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,218 kB]\n",
            "Get:12 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,423 kB]\n",
            "Hit:14 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,134 kB]\n",
            "Hit:16 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,449 kB]\n",
            "Get:18 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [27.8 kB]\n",
            "Get:19 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,553 kB]\n",
            "Fetched 18.3 MB in 2s (9,455 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.11).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 51 not upgraded.\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n",
            "Collecting loguru\n",
            "  Downloading loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: loguru\n",
            "Successfully installed loguru-0.7.2\n",
            "Collecting efficientnet_pytorch\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet_pytorch) (2.3.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet_pytorch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet_pytorch) (1.3.0)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Building wheels for collected packages: efficientnet_pytorch\n",
            "  Building wheel for efficientnet_pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16425 sha256=cd55acd6185f2c50e20304769d4f83422093dec570638b060a047f3893941a1a\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n",
            "Successfully built efficientnet_pytorch\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, efficientnet_pytorch\n",
            "Successfully installed efficientnet_pytorch-0.7.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.17.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.13.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading wandb-0.17.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.13.0-py2.py3-none-any.whl (309 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.1/309.1 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.13.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.7\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.12.3)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.3.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n"
          ]
        }
      ],
      "execution_count": 1,
      "source": [
        "# setup\n",
        "!apt-get update\n",
        "!apt-get install git\n",
        "!pip install python-dotenv\n",
        "!pip install loguru\n",
        "!pip install efficientnet_pytorch\n",
        "!pip install wandb\n",
        "!pip install imbalanced-learn"
      ]
    },
    {
      "metadata": {
        "id": "KdKGr4YuXUnN"
      },
      "cell_type": "markdown",
      "source": [
        "### Clone the repository and add it to python path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clone repo in order to have modules available\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "# Define the parameters\n",
        "username = \"bscheuringer\"\n",
        "access_token = \"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\"\n",
        "repo_name = \"AILS-MICCAI-UWF4DR-Challenge\"\n",
        "repo_clone_url = f\"https://{username}:{access_token}@github.com/moritsih/{repo_name}.git\"\n",
        "repo_path = f'/content/{repo_name}'\n",
        "\n",
        "# Check if the repository already exists\n",
        "if not os.path.isdir(repo_path):\n",
        "    !git clone {repo_clone_url}\n",
        "else:\n",
        "    print(\"Repository already exists.\")\n",
        "\n",
        "# navigate to repo directory in order to have working imports\n",
        "%cd {repo_path}\n",
        "\n",
        "!git checkout bsc-last-minute  # TODO remove when branch is not needed anymore\n",
        "\n",
        "# add repo path to sys path\n",
        "if repo_path not in sys.path:\n",
        "    sys.path.append(repo_path)\n",
        "\n",
        "# Print sys.path to verify\n",
        "print(\"Python Path:\", sys.path)"
      ],
      "metadata": {
        "id": "1Np1qcT1gHgs",
        "outputId": "6de68259-1eb0-4a62-8e85-a1749662a04b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AILS-MICCAI-UWF4DR-Challenge'...\n",
            "remote: Enumerating objects: 1504, done.\u001b[K\n",
            "remote: Counting objects: 100% (86/86), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 1504 (delta 43), reused 52 (delta 23), pack-reused 1418 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1504/1504), 393.92 MiB | 33.74 MiB/s, done.\n",
            "Resolving deltas: 100% (1011/1011), done.\n",
            "Updating files: 100% (121/121), done.\n",
            "/content/AILS-MICCAI-UWF4DR-Challenge\n",
            "Branch 'bsc-last-minute' set up to track remote branch 'bsc-last-minute' from 'origin'.\n",
            "Switched to a new branch 'bsc-last-minute'\n",
            "Python Path: ['/content', '/env/python', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.10/dist-packages/IPython/extensions', '/usr/local/lib/python3.10/dist-packages/setuptools/_vendor', '/root/.ipython', '/content/AILS-MICCAI-UWF4DR-Challenge']\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9SKkq9-XUnN",
        "outputId": "7c19ebdf-cfb4-4046-8705-184693e0282f"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from 'https://drive.google.com/uc?id=1jm48RSCctyxtEkppS45Znh0wtdf9patA' to 'data/downloads/DeepDRiD.zip.enc'\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1jm48RSCctyxtEkppS45Znh0wtdf9patA\n",
            "From (redirected): https://drive.google.com/uc?id=1jm48RSCctyxtEkppS45Znh0wtdf9patA&confirm=t&uuid=722d9967-097d-47b9-b071-49522df64212\n",
            "To: /content/AILS-MICCAI-UWF4DR-Challenge/data/downloads/DeepDRiD.zip.enc\n",
            "100% 303M/303M [00:09<00:00, 31.1MB/s]\n",
            "Downloaded to 'data/downloads/DeepDRiD.zip.enc'\n",
            "Decrypted from 'data/downloads/DeepDRiD.zip.enc' to 'data/downloads/DeepDRiD.zip'\n",
            "Extracting 'data/downloads/DeepDRiD.zip' to 'data/external'\n",
            "Downloading from 'https://drive.google.com/uc?id=1K8xwscXQQo0KXEzFaC2wybgD-UYNXvfc' to 'data/downloads/UWF4DRChallengeData.zip.enc'\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1K8xwscXQQo0KXEzFaC2wybgD-UYNXvfc\n",
            "From (redirected): https://drive.google.com/uc?id=1K8xwscXQQo0KXEzFaC2wybgD-UYNXvfc&confirm=t&uuid=4526b710-b437-40f4-bcba-7062264863db\n",
            "To: /content/AILS-MICCAI-UWF4DR-Challenge/data/downloads/UWF4DRChallengeData.zip.enc\n",
            "100% 150M/150M [00:06<00:00, 23.5MB/s]\n",
            "Downloaded to 'data/downloads/UWF4DRChallengeData.zip.enc'\n",
            "Decrypted from 'data/downloads/UWF4DRChallengeData.zip.enc' to 'data/downloads/UWF4DRChallengeData.zip'\n",
            "Extracting 'data/downloads/UWF4DRChallengeData.zip' to 'data/raw'\n",
            "Downloading from 'https://drive.google.com/uc?id=1t7Dt8ViDAZ4fLYsFBWmU12Smv10hJyLo' to 'data/downloads/automorph_best_loss_checkpoint.pth'\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1t7Dt8ViDAZ4fLYsFBWmU12Smv10hJyLo\n",
            "From (redirected): https://drive.google.com/uc?id=1t7Dt8ViDAZ4fLYsFBWmU12Smv10hJyLo&confirm=t&uuid=cdcf3497-17b9-468e-9149-1572df0bcace\n",
            "To: /content/AILS-MICCAI-UWF4DR-Challenge/data/downloads/automorph_best_loss_checkpoint.pth\n",
            "100% 72.9M/72.9M [00:02<00:00, 25.8MB/s]\n",
            "Downloaded to 'data/downloads/automorph_best_loss_checkpoint.pth'\n",
            "File 'data/downloads/automorph_best_loss_checkpoint.pth' is not encrypted. Skipping decryption.\n",
            "Skipping extraction for 'data/downloads/automorph_best_loss_checkpoint.pth'\n",
            "Moving 'data/downloads/automorph_best_loss_checkpoint.pth' to 'models/AutoMorph'\n",
            "Moved to 'models/AutoMorph/automorph_best_loss_checkpoint.pth'\n",
            "Downloading from 'https://drive.google.com/uc?id=1X5gekt4_BbIZLoj2fVMHwjWRRGWotzm1' to 'data/downloads/model_weights.pth'\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1X5gekt4_BbIZLoj2fVMHwjWRRGWotzm1\n",
            "To: /content/AILS-MICCAI-UWF4DR-Challenge/data/downloads/model_weights.pth\n",
            "100% 85.3M/85.3M [00:02<00:00, 34.1MB/s]\n",
            "Downloaded to 'data/downloads/model_weights.pth'\n",
            "File 'data/downloads/model_weights.pth' is not encrypted. Skipping decryption.\n",
            "Skipping extraction for 'data/downloads/model_weights.pth'\n",
            "File 'models/submission_eval_model_weights/model_weights.pth' already exists. Skipping move.\n"
          ]
        }
      ],
      "execution_count": 3,
      "source": [
        "# load and unzip data\n",
        "!python ./tools/download_data_and_chkpts.py"
      ]
    },
    {
      "metadata": {
        "id": "RQQDSrsDXUnO"
      },
      "cell_type": "markdown",
      "source": [
        "### Optionally resize deepdrid images in order to save computation time\n",
        "NOTE: this will REPLACE original images !"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-24T12:46:52.804813Z",
          "start_time": "2024-07-24T12:45:51.215834Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6rUCRwSXUnO",
        "outputId": "0c22eed4-3c6f-44b7-e7e5-748432af1705"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def resize_image(image_path, size):\n",
        "    with Image.open(image_path) as img:\n",
        "        img = img.resize(size)\n",
        "        img.save(image_path)\n",
        "\n",
        "def process_directory(root_dir, size):\n",
        "    if not os.path.isdir(root_dir):\n",
        "        print(f\"Error: The directory '{root_dir}' does not exist.\")\n",
        "        return\n",
        "\n",
        "    image_count = 0\n",
        "\n",
        "    for dirpath, _, filenames in os.walk(root_dir):\n",
        "        for filename in filenames:\n",
        "            if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.tiff')):\n",
        "                image_path = os.path.join(dirpath, filename)\n",
        "                resize_image(image_path, size)\n",
        "                image_count += 1\n",
        "\n",
        "    print(f\"Processed {image_count} images.\")\n",
        "\n",
        "\n",
        "process_directory('data/external/DeepDRiD', (1016, 800) )"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 256 images.\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "metadata": {
        "id": "HA2thKYHXUnO"
      },
      "cell_type": "markdown",
      "source": [
        "### Verify if project has been successfully cloned and added to python path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test repo import\n",
        "!ls {repo_path}\n",
        "\n",
        "# try importing a custom class\n",
        "try:\n",
        "    from ails_miccai_uwf4dr_challenge.dataset_strategy import Task1Strategy\n",
        "\n",
        "    print(\"Import successful!\")\n",
        "except ImportError as e:\n",
        "    print(\"Import failed:\", e)"
      ],
      "metadata": {
        "id": "j-0jnXXgnArd",
        "outputId": "9e925b53-16e4-4e2e-8734-7c4ae08931f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "ExecuteTime": {
          "end_time": "2024-07-24T12:47:52.015093Z",
          "start_time": "2024-07-24T12:47:45.573653Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aes256.key\t\t      docs\tnotebooks\treferences\t  tests\n",
            "ails_miccai_uwf4dr_challenge  Makefile\tpyproject.toml\treports\t\t  tools\n",
            "data\t\t\t      models\tREADME.md\trequirements.txt  wandb\n",
            "Import successful!\n"
          ]
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gy3zF2c9TqO9",
        "ExecuteTime": {
          "end_time": "2024-08-19T20:00:12.675216Z",
          "start_time": "2024-08-19T20:00:08.646373Z"
        }
      },
      "source": [
        "# imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "\n",
        "from ails_miccai_uwf4dr_challenge.models.metrics import sensitivity_score, specificity_score\n",
        "from ails_miccai_uwf4dr_challenge.models.trainer import Metric, DefaultMetricsEvaluationStrategy, Trainer, TrainingContext, MetricCalculatedHook, PersistBestModelOnEpochEndHook, EpochEndHook, ModelResultsAndLabels"
      ],
      "outputs": [],
      "execution_count": 7
    },
    {
      "metadata": {
        "id": "muTlnyjwTqO9",
        "outputId": "bba17e52-d47b-489a-e60f-635002120a97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "ExecuteTime": {
          "end_time": "2024-08-19T20:00:26.386511Z",
          "start_time": "2024-08-19T20:00:26.370897Z"
        }
      },
      "cell_type": "code",
      "source": [
        "# connect to gDrive\n",
        "from pathlib import Path\n",
        "run_on_colab = True\n",
        "if run_on_colab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    my_data_base_path = Path(\"/content/drive/My Drive/JKU/AILS_CHALLENGE_2024\")\n",
        "else:\n",
        "    my_data_base_path = Path(\"local_runs\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "execution_count": 8
    },
    {
      "metadata": {
        "id": "t0wClbAUTqO-",
        "outputId": "5b6e72b1-8537-4494-f9ef-691a60d7533e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "ExecuteTime": {
          "end_time": "2024-08-19T20:00:29.122479Z",
          "start_time": "2024-08-19T20:00:29.102314Z"
        }
      },
      "cell_type": "code",
      "source": [
        "# select device for training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device: \" + str(device))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "execution_count": 9
    },
    {
      "metadata": {
        "id": "JKxtdcnuhesI",
        "ExecuteTime": {
          "end_time": "2024-08-19T20:00:37.680382Z",
          "start_time": "2024-08-19T20:00:37.662552Z"
        }
      },
      "cell_type": "code",
      "source": [
        "# login to wandb\n",
        "use_wandb = True\n",
        "if use_wandb:\n",
        "    import wandb\n",
        "    #wandb.login()"
      ],
      "outputs": [],
      "execution_count": 10
    },
    {
      "metadata": {
        "id": "gbhyv3dHXUnP"
      },
      "cell_type": "markdown",
      "source": [
        "### Green Channel Enhancement\n",
        "A Hybrid Algorithm to Enhance Colour Retinal Fundus Images Using a Wiener Filter and CLAHE\n",
        "\n",
        "see https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8329119/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from skimage import restoration\n",
        "import torch\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "class GreenChannelEnhancement:\n",
        "    def __call__(self, img):\n",
        "        # Convert to numpy array if it's a tensor\n",
        "        if isinstance(img, torch.Tensor):\n",
        "            img = img.numpy().transpose((1, 2, 0))\n",
        "\n",
        "        # Ensure the image is in the correct format\n",
        "        img = img.astype(np.float32)\n",
        "\n",
        "        # Separate the channels\n",
        "        r, g, b = cv2.split(img)\n",
        "\n",
        "        # Apply Wiener filter to the green channel\n",
        "        psf = np.ones((5, 5)) / 25\n",
        "        g_filtered = restoration.wiener(g, psf, balance=0.1)\n",
        "\n",
        "        # Apply CLAHE to the filtered green channel\n",
        "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "        g_enhanced = clahe.apply((g_filtered * 255).astype(np.uint8))\n",
        "        g_enhanced = g_enhanced / 255.0  # Normalize back to range [0, 1]\n",
        "\n",
        "        # Ensure all channels are the same type\n",
        "        r = r.astype(np.float32)\n",
        "        g_enhanced = g_enhanced.astype(np.float32)\n",
        "        b = b.astype(np.float32)\n",
        "\n",
        "        # Merge the enhanced green channel back with the original red and blue channels\n",
        "        enhanced_img = cv2.merge((r, g_enhanced, b))\n",
        "\n",
        "        # Convert back to tensor\n",
        "        enhanced_img = torch.from_numpy(enhanced_img.transpose((2, 0, 1)))\n",
        "        return enhanced_img"
      ],
      "metadata": {
        "id": "5zRh669DvMSI",
        "ExecuteTime": {
          "end_time": "2024-08-19T20:00:42.599617Z",
          "start_time": "2024-08-19T20:00:40.401706Z"
        }
      },
      "outputs": [],
      "execution_count": 11
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resize with padding (keep ratio)"
      ],
      "metadata": {
        "id": "hio0sYB368qt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Resize with padding in order to keep ratio\n",
        "\n",
        "from PIL import Image  # Correct import statement\n",
        "\n",
        "\n",
        "class ResizeWithPadding:\n",
        "    def __init__(self, size, padding_color=(0, 0, 0)):\n",
        "        self.size = size\n",
        "        self.padding_color = padding_color  # Color for padding (default is black)\n",
        "\n",
        "    def __call__(self, img):\n",
        "        # Resize maintaining aspect ratio\n",
        "        img.thumbnail((self.size[1], self.size[0]))\n",
        "\n",
        "        # Create a new image and paste the resized image onto the center\n",
        "        new_img = Image.new(\"RGB\", self.size, self.padding_color)\n",
        "        new_img.paste(img, ((self.size[0] - img.size[0]) // 2, (self.size[1] - img.size[1]) // 2))\n",
        "\n",
        "        return new_img"
      ],
      "metadata": {
        "id": "XsrMGJQV5w4R"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VqMmN4X5XUnP"
      },
      "cell_type": "markdown",
      "source": [
        "### Model Definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Efficient Net B0"
      ],
      "metadata": {
        "id": "tSN_pUQZ6xo0"
      }
    },
    {
      "metadata": {
        "id": "mnaQQSUtsH2P",
        "ExecuteTime": {
          "end_time": "2024-07-23T15:10:19.782651Z",
          "start_time": "2024-07-23T15:10:19.758373Z"
        }
      },
      "cell_type": "code",
      "source": [
        "# EfficientNet B0\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "class Task1EfficientNetB0(nn.Module):\n",
        "    def __init__(self, learning_rate=1e-3):\n",
        "        super(Task1EfficientNetB0, self).__init__()\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Get model and replace the last layer\n",
        "        self.model = EfficientNet.from_pretrained('efficientnet-b0', num_classes=1)\n",
        "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        # Freeze all layers except the last one\n",
        "        #for param in self.model.parameters():\n",
        "        #    param.requires_grad = False\n",
        "\n",
        "        # Unfreeze the last layer\n",
        "        #for param in self.model._fc.parameters():\n",
        "        #    param.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def predict(self, x):\n",
        "        with torch.no_grad():\n",
        "            pred = torch.sigmoid(self(x))\n",
        "        return pred"
      ],
      "outputs": [],
      "execution_count": 14
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Efficient Net B0 with explicit Blur Feature"
      ],
      "metadata": {
        "id": "K-yVJDcv6L55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EfficientNetB0 with explicit blur feature\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "class Task1EfficientNetB0_WithExplicitBlur(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Task1EfficientNetB0_WithExplicitBlur, self).__init__()\n",
        "\n",
        "        # Load the pre-trained EfficientNetB0 model\n",
        "        self.model = EfficientNet.from_pretrained('efficientnet-b0', num_classes=1)\n",
        "\n",
        "        # Modify the final fully connected layer to accommodate the additional input\n",
        "        self.model._fc = nn.Linear(self.model._fc.in_features + 1, 1)  # Add 1 for the blurriness score\n",
        "\n",
        "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def calculate_blurriness(self, img):\n",
        "        \"\"\"Calculate the blurriness of an image using the variance of the Laplacian.\"\"\"\n",
        "        img_np = img.permute(1, 2, 0).cpu().numpy()  # Convert tensor to NumPy array in HWC format\n",
        "        img_np = (img_np * 255).astype(np.uint8)  # Convert from [0, 1] to [0, 255]\n",
        "        gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n",
        "        laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "        return laplacian_var\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Calculate blurriness on the fly for each image in the batch\n",
        "        blurriness_scores = torch.tensor([self.calculate_blurriness(img) for img in x], dtype=torch.float32).unsqueeze(1).to(x.device)\n",
        "\n",
        "        # Extract features from the image using EfficientNet\n",
        "        x = self.model.extract_features(x)\n",
        "        x = self.model._avg_pooling(x)\n",
        "        x = x.flatten(start_dim=1)\n",
        "        x = self.model._dropout(x)\n",
        "\n",
        "        # Concatenate blurriness score with the extracted features\n",
        "        x = torch.cat((x, blurriness_scores), dim=1)\n",
        "\n",
        "        # Final classification layer\n",
        "        x = self.model._fc(x)\n",
        "        return x\n",
        "\n",
        "    def predict(self, x):\n",
        "        with torch.no_grad():\n",
        "            pred = torch.sigmoid(self(x))\n",
        "        return pred\n"
      ],
      "metadata": {
        "id": "m_Y2ytdL4u0X"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Efficient Net B0 with Self Attention"
      ],
      "metadata": {
        "id": "_-LNLKcU6SEZ"
      }
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-08-19T20:18:04.625764Z",
          "start_time": "2024-08-19T20:18:04.595542Z"
        },
        "id": "UuDF54BVXUnQ"
      },
      "cell_type": "code",
      "source": [
        "# Attention Efficient Net\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, in_dim):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.query_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
        "        self.key_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
        "        self.value_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        m_batchsize, C, width, height = x.size()\n",
        "        proj_query = self.query_conv(x).view(m_batchsize, -1, width*height).permute(0, 2, 1)\n",
        "        proj_key = self.key_conv(x).view(m_batchsize, -1, width*height)\n",
        "        energy = torch.bmm(proj_query, proj_key)\n",
        "        attention = self.softmax(energy)\n",
        "        proj_value = self.value_conv(x).view(m_batchsize, -1, width*height)\n",
        "\n",
        "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
        "        out = out.view(m_batchsize, C, width, height)\n",
        "\n",
        "        out = self.gamma * out + x\n",
        "        return out\n",
        "\n",
        "class AttentionEfficientNetB0(nn.Module):\n",
        "    def __init__(self, num_classes=1, pretrained=True):\n",
        "        super(AttentionEfficientNetB0, self).__init__()\n",
        "        self.model = EfficientNet.from_pretrained('efficientnet-b0', num_classes=num_classes) if pretrained else EfficientNet.from_name('efficientnet-b0', num_classes=num_classes)\n",
        "\n",
        "        # Adjust Self-Attention layers to match the output channels of the corresponding blocks\n",
        "        self.attention1 = SelfAttention(in_dim=16)  # Match in_dim with the output channels of _blocks[0]\n",
        "        self.attention2 = SelfAttention(in_dim=40)  # Match in_dim with the output channels of _blocks[3]\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extracting blocks from EfficientNet\n",
        "        x = self.model._conv_stem(x)  # Output: 32 channels\n",
        "        x = self.model._bn0(x)\n",
        "        x = self.model._swish(x)\n",
        "\n",
        "        # First block with attention (after _blocks[0])\n",
        "        x = self.model._blocks[0](x)  # Output: 16 channels\n",
        "        x = self.attention1(x)\n",
        "\n",
        "        # Continue with EfficientNet layers and apply second attention after _blocks[3]\n",
        "        for idx, block in enumerate(self.model._blocks[1:]):\n",
        "            x = block(x)\n",
        "            if idx == 3:  # Apply attention after _blocks[3] (which outputs 40 channels)\n",
        "                x = self.attention2(x)\n",
        "\n",
        "        # Finish with the remaining EfficientNet layers\n",
        "        x = self.model._conv_head(x)\n",
        "        x = self.model._bn1(x)\n",
        "        x = self.model._swish(x)\n",
        "        x = self.model._avg_pooling(x)\n",
        "        x = x.flatten(start_dim=1)\n",
        "        x = self.model._dropout(x)\n",
        "        x = self.model._fc(x)\n",
        "\n",
        "        return x\n"
      ],
      "outputs": [],
      "execution_count": 32
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Automorph pre trained model (Efficient Net B4)"
      ],
      "metadata": {
        "id": "V0AJihio6bn3"
      }
    },
    {
      "metadata": {
        "id": "ZXRqU-xbXUnQ"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Automorph\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "class AutoMorphModel(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super(AutoMorphModel, self).__init__()\n",
        "\n",
        "        # code taken from https://github.com/rmaphoh/AutoMorph/blob/main/M1_Retinal_Image_quality_EyePACS/model.py\n",
        "        self.model = EfficientNet.from_pretrained('efficientnet-b4')\n",
        "        self.model._fc = nn.Identity()\n",
        "        net_fl = nn.Sequential(\n",
        "            nn.Linear(1792, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(256, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(64, 3)\n",
        "        )\n",
        "        self.model._fc = net_fl\n",
        "        if pretrained:\n",
        "            checkpoint_path = Path().resolve() / \"models\" / \"AutoMorph\" / \"automorph_best_loss_checkpoint.pth\"\n",
        "            self.model.load_state_dict(torch.load(checkpoint_path, map_location='cpu'))\n",
        "            print(f\"Loaded pretrained Automorph model checkpoint from {checkpoint_path}\")\n",
        "\n",
        "        # add a final layer that outputs single value\n",
        "        self.model._fc.add_module(\"7\", nn.Linear(3, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Efficient Net B0 with extended classifier"
      ],
      "metadata": {
        "id": "z0KkhoYj6gPy"
      }
    },
    {
      "metadata": {
        "id": "vVEWSpKRsH2Q",
        "ExecuteTime": {
          "end_time": "2024-07-24T13:03:40.276148Z",
          "start_time": "2024-07-24T13:03:40.240493Z"
        }
      },
      "cell_type": "code",
      "source": [
        "# EfficientNet0 with extended classifier\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "class Task1EfficientNetB0Extended(nn.Module):\n",
        "    def __init__(self, learning_rate=1e-3):\n",
        "        super(Task1EfficientNetB0Extended, self).__init__()\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Get model and replace the last layer\n",
        "        self.model = EfficientNet.from_pretrained('efficientnet-b0')\n",
        "\n",
        "        # Determine the number of input features for the classifier\n",
        "        in_features = self.model._fc.in_features\n",
        "\n",
        "        # Replace the last layer with a custom classifier block\n",
        "        self.model._fc = nn.Sequential(\n",
        "            nn.Linear(in_features, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.3),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(512, 64),\n",
        "            nn.ReLU(),\n",
        "            #nn.Dropout(p=0.4),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        # Freeze all layers except the last one\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Unfreeze the last layer\n",
        "        for param in self.model._fc.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def predict(self, x):\n",
        "        with torch.no_grad():\n",
        "            pred = torch.sigmoid(self(x))\n",
        "        return pred\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Efficient NetV2"
      ],
      "metadata": {
        "id": "IwUdxD436kva"
      }
    },
    {
      "metadata": {
        "id": "Hc1KUX7KsH2Q",
        "ExecuteTime": {
          "end_time": "2024-07-23T15:10:23.154792Z",
          "start_time": "2024-07-23T15:10:23.130373Z"
        }
      },
      "cell_type": "code",
      "source": [
        "# EfficientNetV2\n",
        "\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import efficientnet_v2_s\n",
        "\n",
        "class Task1EfficientNetV2(nn.Module):\n",
        "    def __init__(self, learning_rate=1e-3):\n",
        "        super(Task1EfficientNetV2, self).__init__()\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Get the EfficientNetV2 model\n",
        "        self.model = efficientnet_v2_s(weights=\"IMAGENET1K_V1\")\n",
        "\n",
        "        # Replace the entire classifier block\n",
        "        in_features = self.model.classifier[1].in_features\n",
        "        self.model.classifier = nn.Sequential(\n",
        "            nn.Linear(in_features, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(512, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.4),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def predict(self, x):\n",
        "        with torch.no_grad():\n",
        "            pred = torch.sigmoid(self(x))\n",
        "        return pred\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CLAHE Transformation"
      ],
      "metadata": {
        "id": "Z4D3GUvJ6p_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "class CLAHETransform:\n",
        "    def __init__(self, clip_limit=2.0, tile_grid_size=(8, 8)):\n",
        "        self.clip_limit = clip_limit\n",
        "        self.tile_grid_size = tile_grid_size\n",
        "\n",
        "    def __call__(self, img):\n",
        "        # Convert PIL Image to NumPy array\n",
        "        img = np.array(img)\n",
        "\n",
        "        # Convert the image to LAB color space\n",
        "        lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
        "\n",
        "        # Split the LAB image to different channels\n",
        "        l, a, b = cv2.split(lab)\n",
        "\n",
        "        # Apply CLAHE to the L channel\n",
        "        clahe = cv2.createCLAHE(clipLimit=self.clip_limit, tileGridSize=self.tile_grid_size)\n",
        "        cl = clahe.apply(l)\n",
        "\n",
        "        # Merge the CLAHE enhanced L channel back with A and B channels\n",
        "        limg = cv2.merge((cl, a, b))\n",
        "\n",
        "        # Convert the image back to RGB color space\n",
        "        final_img = cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)\n",
        "\n",
        "        # Convert back to PIL Image\n",
        "        return Image.fromarray(final_img)"
      ],
      "metadata": {
        "id": "hJmNGAChdxOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Oroj7VVWXUnQ"
      },
      "cell_type": "markdown",
      "source": [
        "### Transformations"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-08-19T20:18:41.974819Z",
          "start_time": "2024-08-19T20:18:41.960462Z"
        },
        "id": "7lSvqLftXUnR"
      },
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "import torch\n",
        "\n",
        "augment_for_task_1_training = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    ResizeWithPadding(size=(224, 224)),\n",
        "    #CLAHETransform(clip_limit=2.0, tile_grid_size=(8, 8)),  # Apply CLAHE\n",
        "    transforms.ToTensor(),  # Convert to float32 tensor and scale\n",
        "    #GreenChannelEnhancement(),  # Apply Wiener filter and CLAHE\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    #transforms.RandomRotation(degrees=5, expand=True),\n",
        "    # transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "    #transforms.Resize(size=(224, 224)),\n",
        "    transforms.Normalize(mean=[0.406, 0.456, 0.485], std=[0.225, 0.224, 0.229])\n",
        "])\n",
        "\n",
        "# Augmentation pipeline for validation\n",
        "augment_for_task_1_validation = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    ResizeWithPadding(size=(224, 224)),\n",
        "    #CLAHETransform(clip_limit=2.0, tile_grid_size=(8, 8)),  # Apply CLAHE\n",
        "    transforms.ToTensor(),  # Convert to float32 tensor and scale\n",
        "    #GreenChannelEnhancement(),  # Apply Wiener filter and CLAHE\n",
        "    #transforms.Resize(size=(224, 224)),\n",
        "    transforms.Normalize(mean=[0.406, 0.456, 0.485], std=[0.225, 0.224, 0.229])\n",
        "])"
      ],
      "outputs": [],
      "execution_count": 33
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOgPQ5ZKsH2Q",
        "outputId": "743c57ef-9e81-4b40-b059-26fc794a6357",
        "ExecuteTime": {
          "end_time": "2024-08-19T20:18:44.425592Z",
          "start_time": "2024-08-19T20:18:44.290659Z"
        }
      },
      "cell_type": "code",
      "source": [
        "#model = Task1EfficientNetB0(1e-4)\n",
        "\n",
        "#model = Task1EfficientNetB0_WithExplicitBlur()\n",
        "\n",
        "\n",
        "model = AttentionEfficientNetB0()\n",
        "\n",
        "#model = AutoMorphModel(pretrained=True)\n",
        "\n",
        "#model = Task1EfficientNetB0Extended(1e-4)\n",
        "\n",
        "#state_dict = model.load_state_dict(torch.load(my_data_base_path / 'Task1EfficientNetB0Extended_best_weights_2024-07-23_06-34-11_tough-cosmos-713.pth', map_location='cpu'))\n",
        "\n",
        "#model = Task1EfficientNetV2()\n",
        "\n",
        "\n",
        "model.to(device)\n",
        "model_name = model.__class__.__name__"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained weights for efficientnet-b0\n"
          ]
        }
      ],
      "execution_count": 34
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-08-19T20:18:46.645157Z",
          "start_time": "2024-08-19T20:18:46.624183Z"
        },
        "id": "aMYSaM9yXUnR"
      },
      "cell_type": "code",
      "source": [
        "# setup dataset\n",
        "from ails_miccai_uwf4dr_challenge.dataset_strategy import OriginalDatasetStrategy, Task1Strategy, DatasetBuilder, CustomDataset\n",
        "\n",
        "# setup dataset\n",
        "\n",
        "dataset_strategy = OriginalDatasetStrategy() # Original data only\n",
        "task_strategy = Task1Strategy() #TASK 1\n",
        "\n",
        "\n",
        "# Build dataset\n",
        "dataset_builder = DatasetBuilder(dataset_strategy, task_strategy, split_ratio=0.8)"
      ],
      "outputs": [],
      "execution_count": 35
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkuNXL9zTqO-",
        "outputId": "ef5871f9-4f1c-423b-97d0-906a43cf3b11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "ExecuteTime": {
          "end_time": "2024-08-19T20:18:48.613156Z",
          "start_time": "2024-08-19T20:18:47.609132Z"
        }
      },
      "source": [
        "# training config\n",
        "print(\"Training model: \",model_name)\n",
        "\n",
        "metrics = [\n",
        "        Metric('auroc', roc_auc_score),\n",
        "        Metric('auprc', average_precision_score),\n",
        "        Metric('accuracy', lambda y_true, y_pred: (y_pred.round() == y_true).mean()),\n",
        "        Metric('sensitivity', sensitivity_score),\n",
        "        Metric('specificity', specificity_score)\n",
        "    ]\n",
        "\n",
        "class WandbLoggingHook(MetricCalculatedHook):\n",
        "        def on_metric_calculated(self, training_context: TrainingContext, metric: Metric, result, last_metric_for_epoch: bool):\n",
        "            import wandb\n",
        "            wandb.log(data={metric.name: result}, commit=last_metric_for_epoch)\n",
        "\n",
        "class FreezeSwitchHook(EpochEndHook):\n",
        "    def __init__(self, unfreeze_on_epoch=5):\n",
        "        self.epoch_cnt = 0\n",
        "        self.unfreeze_on_epoch = unfreeze_on_epoch\n",
        "\n",
        "    def on_epoch_end(self, training_context: TrainingContext, train_results: ModelResultsAndLabels, val_results: ModelResultsAndLabels):\n",
        "        self.epoch_cnt += 1\n",
        "        if self.epoch_cnt == self.unfreeze_on_epoch:\n",
        "            for param in model.model.parameters():\n",
        "                param.requires_grad = True\n",
        "            print(f\"Unfreezing all layers on epoch [{self.epoch_cnt}].\")\n",
        "\n",
        "\n",
        "metrics_eval_strategy = DefaultMetricsEvaluationStrategy(metrics)\n",
        "\n",
        "if(use_wandb):\n",
        "    metrics_eval_strategy.register_metric_calculated_hook(WandbLoggingHook())\n",
        "\n",
        "config = {\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"dataset\": dataset_strategy.__class__.__name__,\n",
        "    \"epochs\": 25,\n",
        "    \"batch_size\": 4,\n",
        "    \"model_type\": model.__class__.__name__\n",
        "}\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=config[\"learning_rate\"])\n",
        "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
        "\n",
        "train_data, val_data = dataset_builder.build()\n",
        "train_dataset = CustomDataset(train_data, transform=augment_for_task_1_training)\n",
        "val_dataset = CustomDataset(val_data, transform=augment_for_task_1_validation)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=4)\n",
        "\n",
        "\n",
        "trainer = Trainer(model, train_loader, val_loader, criterion, optimizer, lr_scheduler, device,\n",
        "                        metrics_eval_strategy=metrics_eval_strategy)\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model:  AttentionEfficientNetB0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
          ]
        }
      ],
      "execution_count": 36
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c60c2c93972b4539a848bcdaa5c4efea",
            "5cfd259607614f5a83afab3ca9929c02",
            "122cce0e48b441839084fca48492e009",
            "f8acc357af1d4242a6bcfc63ef595459",
            "d21b6932e0e5452fa8bcd043987ef1a5",
            "d975aa6ea38340fca4f5108d51a8905a",
            "4fea7c949b8b451eb57337498c111b08",
            "e4f694aa6435427ab89584177cf52fc3"
          ]
        },
        "id": "GCNm2VYHsH2R",
        "outputId": "887fec2a-d469-4644-d963-c2bdbfae6ec5",
        "ExecuteTime": {
          "end_time": "2024-08-19T20:21:15.951327Z",
          "start_time": "2024-08-19T20:18:49.163537Z"
        }
      },
      "cell_type": "code",
      "source": [
        "# start training\n",
        "\n",
        "if use_wandb:\n",
        "    wandb.init(entity='miccai-challenge-2024' ,project='task1', config=config)\n",
        "    postfix = wandb.run.name\n",
        "    print(f'wandb run: {wandb.run.name}')\n",
        "else:\n",
        "    postfix = time.strftime(\"%H-%M-%S\")\n",
        "\n",
        "print(f\"Start training [{config['model_type']}] on [{config['dataset']}] dataset for [{config['epochs']}] epochs with batch size [{config['batch_size']}]\")\n",
        "\n",
        "# build a file name for the model weights containing current timestamp and the model class\n",
        "training_date = time.strftime(\"%Y-%m-%d\")\n",
        "persist_model_hook = PersistBestModelOnEpochEndHook(my_data_base_path / f\"{model_name}_weights_{training_date}_{postfix}.pth\", print_train_results=True)\n",
        "trainer.add_epoch_end_hook(persist_model_hook)\n",
        "#trainer.add_epoch_end_hook(FreezeSwitchHook(unfreeze_on_epoch=5))\n",
        "\n",
        "trainer.train(num_epochs=config[\"epochs\"])\n",
        "print(\"Training finished.\")\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:wp1odd9b) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c60c2c93972b4539a848bcdaa5c4efea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▅▄▆▆▅▇▆▇▆▇▆█▇▆▆▆▆▇█▆▇▇▇█</td></tr><tr><td>auprc</td><td>▁▇▇█▇██████▇████▇████████</td></tr><tr><td>auroc</td><td>▁▇▇█▇█████▇▇████▇█████▇██</td></tr><tr><td>avg_train_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>avg_val_loss</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sensitivity</td><td>▇█▂▁▄▇▄▂▅█▄▂▅▂▅▅▁▅▅▇▄▇▂▅▅</td></tr><tr><td>specificity</td><td>▁▅▇██▇██▇▇███████▇▇██████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.90805</td></tr><tr><td>auprc</td><td>0.9688</td></tr><tr><td>auroc</td><td>0.95069</td></tr><tr><td>avg_train_loss</td><td>0.02999</td></tr><tr><td>avg_val_loss</td><td>0.28162</td></tr><tr><td>sensitivity</td><td>0.86957</td></tr><tr><td>specificity</td><td>1.0</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">unique-butterfly-1861</strong> at: <a href='https://wandb.ai/miccai-challenge-2024/task1/runs/wp1odd9b' target=\"_blank\">https://wandb.ai/miccai-challenge-2024/task1/runs/wp1odd9b</a><br/> View project at: <a href='https://wandb.ai/miccai-challenge-2024/task1' target=\"_blank\">https://wandb.ai/miccai-challenge-2024/task1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240820_084348-wp1odd9b/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:wp1odd9b). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/AILS-MICCAI-UWF4DR-Challenge/wandb/run-20240820_084840-e4h1k274</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/miccai-challenge-2024/task1/runs/e4h1k274' target=\"_blank\">hopeful-blaze-1862</a></strong> to <a href='https://wandb.ai/miccai-challenge-2024/task1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/miccai-challenge-2024/task1' target=\"_blank\">https://wandb.ai/miccai-challenge-2024/task1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/miccai-challenge-2024/task1/runs/e4h1k274' target=\"_blank\">https://wandb.ai/miccai-challenge-2024/task1/runs/e4h1k274</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wandb run: hopeful-blaze-1862\n",
            "Start training [AttentionEfficientNetB0] on [OriginalDatasetStrategy] dataset for [25] epochs with batch size [4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/25 - Avg train Loss: 0.567540: 100%|██████████| 87/87 [00:22<00:00,  3.84it/s]\n",
            "Epoch 1/25 - Avg val Loss: 0.439875: 100%|██████████| 22/22 [00:01<00:00, 14.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best weights found at epoch 1 with validation loss: 0.4399. Model saved to /content/drive/My Drive/JKU/AILS_CHALLENGE_2024/AttentionEfficientNetB0_weights_2024-08-20_hopeful-blaze-1862.pth\n",
            "Epoch 1/25 Summary : Train Loss: 0.5675, Val Loss: 0.4399, LR: 1.00e-04, auroc: 0.9449, auprc: 0.9614, accuracy: 0.8391, sensitivity: 0.8261, specificity: 0.9512, avg_train_loss: 0.5675, avg_val_loss: 0.4399\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/25 - Avg train Loss: 0.370430: 100%|██████████| 87/87 [00:22<00:00,  3.85it/s]\n",
            "Epoch 2/25 - Avg val Loss: 0.300647: 100%|██████████| 22/22 [00:01<00:00, 14.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best weights found at epoch 2 with validation loss: 0.3006. Model saved to /content/drive/My Drive/JKU/AILS_CHALLENGE_2024/AttentionEfficientNetB0_weights_2024-08-20_hopeful-blaze-1862.pth\n",
            "Epoch 2/25 Summary : Train Loss: 0.3704, Val Loss: 0.3006, LR: 1.00e-04, auroc: 0.9565, auprc: 0.9698, accuracy: 0.8851, sensitivity: 0.8261, specificity: 0.9756, avg_train_loss: 0.3704, avg_val_loss: 0.3006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/25 - Avg train Loss: 0.351012: 100%|██████████| 87/87 [00:22<00:00,  3.86it/s]\n",
            "Epoch 3/25 - Avg val Loss: 0.281038: 100%|██████████| 22/22 [00:01<00:00, 14.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best weights found at epoch 3 with validation loss: 0.2810. Model saved to /content/drive/My Drive/JKU/AILS_CHALLENGE_2024/AttentionEfficientNetB0_weights_2024-08-20_hopeful-blaze-1862.pth\n",
            "Epoch 3/25 Summary : Train Loss: 0.3510, Val Loss: 0.2810, LR: 1.00e-04, auroc: 0.9544, auprc: 0.9679, accuracy: 0.8736, sensitivity: 0.8261, specificity: 0.9756, avg_train_loss: 0.3510, avg_val_loss: 0.2810\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/25 - Avg train Loss: 0.294687: 100%|██████████| 87/87 [00:22<00:00,  3.87it/s]\n",
            "Epoch 4/25 - Avg val Loss: 0.277868: 100%|██████████| 22/22 [00:01<00:00, 14.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best weights found at epoch 4 with validation loss: 0.2779. Model saved to /content/drive/My Drive/JKU/AILS_CHALLENGE_2024/AttentionEfficientNetB0_weights_2024-08-20_hopeful-blaze-1862.pth\n",
            "Epoch 4/25 Summary : Train Loss: 0.2947, Val Loss: 0.2779, LR: 1.00e-04, auroc: 0.9549, auprc: 0.9687, accuracy: 0.8736, sensitivity: 0.8478, specificity: 0.9756, avg_train_loss: 0.2947, avg_val_loss: 0.2779\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/25 - Avg train Loss: 0.300741: 100%|██████████| 87/87 [00:22<00:00,  3.87it/s]\n",
            "Epoch 5/25 - Avg val Loss: 0.335113: 100%|██████████| 22/22 [00:01<00:00, 14.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/25 Summary : Train Loss: 0.3007, Val Loss: 0.3351, LR: 1.00e-04, auroc: 0.9602, auprc: 0.9704, accuracy: 0.8621, sensitivity: 0.8261, specificity: 0.9756, avg_train_loss: 0.3007, avg_val_loss: 0.3351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/25 - Avg train Loss: 0.267209: 100%|██████████| 87/87 [00:22<00:00,  3.87it/s]\n",
            "Epoch 6/25 - Avg val Loss: 0.248053: 100%|██████████| 22/22 [00:01<00:00, 14.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best weights found at epoch 6 with validation loss: 0.2481. Model saved to /content/drive/My Drive/JKU/AILS_CHALLENGE_2024/AttentionEfficientNetB0_weights_2024-08-20_hopeful-blaze-1862.pth\n",
            "Epoch 6/25 Summary : Train Loss: 0.2672, Val Loss: 0.2481, LR: 1.00e-04, auroc: 0.9586, auprc: 0.9721, accuracy: 0.9195, sensitivity: 0.8913, specificity: 0.9756, avg_train_loss: 0.2672, avg_val_loss: 0.2481\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/25 - Avg train Loss: 0.224388: 100%|██████████| 87/87 [00:22<00:00,  3.87it/s]\n",
            "Epoch 7/25 - Avg val Loss: 0.304279: 100%|██████████| 22/22 [00:01<00:00, 14.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/25 Summary : Train Loss: 0.2244, Val Loss: 0.3043, LR: 1.00e-04, auroc: 0.9608, auprc: 0.9705, accuracy: 0.8391, sensitivity: 0.8478, specificity: 0.9512, avg_train_loss: 0.2244, avg_val_loss: 0.3043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/25 - Avg train Loss: 0.161518: 100%|██████████| 87/87 [00:22<00:00,  3.87it/s]\n",
            "Epoch 8/25 - Avg val Loss: 0.249927: 100%|██████████| 22/22 [00:01<00:00, 14.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/25 Summary : Train Loss: 0.1615, Val Loss: 0.2499, LR: 1.00e-04, auroc: 0.9645, auprc: 0.9747, accuracy: 0.9080, sensitivity: 0.9348, specificity: 0.9024, avg_train_loss: 0.1615, avg_val_loss: 0.2499\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/25 - Avg train Loss: 0.136891: 100%|██████████| 87/87 [00:22<00:00,  3.87it/s]\n",
            "Epoch 9/25 - Avg val Loss: 0.244745: 100%|██████████| 22/22 [00:01<00:00, 14.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best weights found at epoch 9 with validation loss: 0.2447. Model saved to /content/drive/My Drive/JKU/AILS_CHALLENGE_2024/AttentionEfficientNetB0_weights_2024-08-20_hopeful-blaze-1862.pth\n",
            "Epoch 9/25 Summary : Train Loss: 0.1369, Val Loss: 0.2447, LR: 1.00e-04, auroc: 0.9698, auprc: 0.9787, accuracy: 0.9080, sensitivity: 0.9130, specificity: 0.9512, avg_train_loss: 0.1369, avg_val_loss: 0.2447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/25 - Avg train Loss: 0.159960: 100%|██████████| 87/87 [00:22<00:00,  3.87it/s]\n",
            "Epoch 10/25 - Avg val Loss: 0.269954: 100%|██████████| 22/22 [00:01<00:00, 14.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/25 Summary : Train Loss: 0.1600, Val Loss: 0.2700, LR: 1.00e-04, auroc: 0.9650, auprc: 0.9736, accuracy: 0.8851, sensitivity: 0.9348, specificity: 0.9024, avg_train_loss: 0.1600, avg_val_loss: 0.2700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/25 - Avg train Loss: 0.114806: 100%|██████████| 87/87 [00:22<00:00,  3.87it/s]\n",
            "Epoch 11/25 - Avg val Loss: 0.252861: 100%|██████████| 22/22 [00:01<00:00, 14.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/25 Summary : Train Loss: 0.1148, Val Loss: 0.2529, LR: 1.00e-04, auroc: 0.9666, auprc: 0.9739, accuracy: 0.8851, sensitivity: 0.8043, specificity: 1.0000, avg_train_loss: 0.1148, avg_val_loss: 0.2529\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/25 - Avg train Loss: 0.114105: 100%|██████████| 87/87 [00:22<00:00,  3.87it/s]\n",
            "Epoch 12/25 - Avg val Loss: 0.223221: 100%|██████████| 22/22 [00:01<00:00, 14.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best weights found at epoch 12 with validation loss: 0.2232. Model saved to /content/drive/My Drive/JKU/AILS_CHALLENGE_2024/AttentionEfficientNetB0_weights_2024-08-20_hopeful-blaze-1862.pth\n",
            "Epoch 12/25 Summary : Train Loss: 0.1141, Val Loss: 0.2232, LR: 1.00e-04, auroc: 0.9724, auprc: 0.9805, accuracy: 0.9080, sensitivity: 0.8696, specificity: 1.0000, avg_train_loss: 0.1141, avg_val_loss: 0.2232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/25 - Avg train Loss: 0.067171: 100%|██████████| 87/87 [00:22<00:00,  3.87it/s]\n",
            "Epoch 13/25 - Avg val Loss: 0.267818: 100%|██████████| 22/22 [00:01<00:00, 14.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/25 Summary : Train Loss: 0.0672, Val Loss: 0.2678, LR: 1.00e-04, auroc: 0.9698, auprc: 0.9776, accuracy: 0.8851, sensitivity: 0.9130, specificity: 0.9512, avg_train_loss: 0.0672, avg_val_loss: 0.2678\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/25 - Avg train Loss: 0.074609: 100%|██████████| 87/87 [00:22<00:00,  3.88it/s]\n",
            "Epoch 14/25 - Avg val Loss: 0.226334: 100%|██████████| 22/22 [00:01<00:00, 14.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/25 Summary : Train Loss: 0.0746, Val Loss: 0.2263, LR: 1.00e-04, auroc: 0.9671, auprc: 0.9761, accuracy: 0.9080, sensitivity: 0.8478, specificity: 1.0000, avg_train_loss: 0.0746, avg_val_loss: 0.2263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/25 - Avg train Loss: 0.068614: 100%|██████████| 87/87 [00:22<00:00,  3.87it/s]\n",
            "Epoch 15/25 - Avg val Loss: 0.326541: 100%|██████████| 22/22 [00:01<00:00, 14.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/25 Summary : Train Loss: 0.0686, Val Loss: 0.3265, LR: 1.00e-04, auroc: 0.9528, auprc: 0.9691, accuracy: 0.8966, sensitivity: 0.8478, specificity: 1.0000, avg_train_loss: 0.0686, avg_val_loss: 0.3265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/25 - Avg train Loss: 0.089379: 100%|██████████| 87/87 [00:22<00:00,  3.87it/s]\n",
            "Epoch 16/25 - Avg val Loss: 0.233050: 100%|██████████| 22/22 [00:01<00:00, 14.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/25 Summary : Train Loss: 0.0894, Val Loss: 0.2331, LR: 1.00e-04, auroc: 0.9671, auprc: 0.9768, accuracy: 0.9195, sensitivity: 0.9130, specificity: 0.9268, avg_train_loss: 0.0894, avg_val_loss: 0.2331\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/25 - Avg train Loss: 0.041219: 100%|██████████| 87/87 [00:22<00:00,  3.87it/s]\n",
            "Epoch 17/25 - Avg val Loss: 0.378625: 100%|██████████| 22/22 [00:01<00:00, 14.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/25 Summary : Train Loss: 0.0412, Val Loss: 0.3786, LR: 1.00e-04, auroc: 0.9602, auprc: 0.9727, accuracy: 0.8736, sensitivity: 0.8913, specificity: 0.9756, avg_train_loss: 0.0412, avg_val_loss: 0.3786\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/25 - Avg train Loss: 0.040698: 100%|██████████| 87/87 [00:22<00:00,  3.88it/s]\n",
            "Epoch 18/25 - Avg val Loss: 0.312844: 100%|██████████| 22/22 [00:01<00:00, 14.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/25 Summary : Train Loss: 0.0407, Val Loss: 0.3128, LR: 5.00e-05, auroc: 0.9512, auprc: 0.9650, accuracy: 0.9080, sensitivity: 0.9130, specificity: 0.9024, avg_train_loss: 0.0407, avg_val_loss: 0.3128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/25 - Avg train Loss: 0.035658: 100%|██████████| 87/87 [00:22<00:00,  3.87it/s]\n",
            "Epoch 19/25 - Avg val Loss: 0.290129: 100%|██████████| 22/22 [00:01<00:00, 14.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/25 Summary : Train Loss: 0.0357, Val Loss: 0.2901, LR: 5.00e-05, auroc: 0.9639, auprc: 0.9733, accuracy: 0.8966, sensitivity: 0.8478, specificity: 0.9756, avg_train_loss: 0.0357, avg_val_loss: 0.2901\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/25 - Avg train Loss: 0.014127:   9%|▉         | 8/87 [00:02<00:20,  3.83it/s]"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "WIYWfXzxXUnR"
      },
      "cell_type": "markdown",
      "source": [
        "### Build Codalab submission (zip) file"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-24T13:10:32.944638Z",
          "start_time": "2024-07-24T13:10:30.375734Z"
        },
        "id": "yFYX90-DXUnR",
        "outputId": "070af75d-b7d6-4c8b-a52c-a3234bdf6d5d"
      },
      "cell_type": "code",
      "source": [
        "# build a submission zip file\n",
        "from tools.build_submission import SubmissionBuilder\n",
        "\n",
        "path_to_codalab_model_file = './models/task_1_EfficientNet0/model.py'\n",
        "path_to_checkpoint_file = persist_model_hook.save_path\n",
        "submission_zip_label = f\"{model_name}_submission_{training_date}_{postfix}\"\n",
        "output_dir = my_data_base_path\n",
        "\n",
        "builder = SubmissionBuilder(model_file=path_to_codalab_model_file, checkpoint_file=path_to_checkpoint_file, label=submission_zip_label, output_dir=output_dir)\n",
        "builder.create_submission_zip()"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing instantiating the model and predict based on a random image...\n",
            "Loaded pretrained weights for efficientnet-b0\n",
            "Prediction on random image: 0.5337051153182983\n",
            "Creating submission zip file 'local_runs\\Task1EfficientNetB0Extended_submission_2024-07-24_15-04-17.zip'\n",
            "-- Adding file metadata\n",
            "-- Adding file model.py\n",
            "-- Adding file Task1EfficientNetB0Extended_weights_2024-07-24_15-04-17.pth\n",
            "Submission zip file 'local_runs\\Task1EfficientNetB0Extended_submission_2024-07-24_15-04-17.zip' created successfully.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "vF_s1zaiXUnS"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": []
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351,
          "referenced_widgets": [
            "7f6010ac490a42a09d9c764cd126d813",
            "3a18f77ae2eb48fc89924cc7660e4143",
            "14501cc83baf447bb325e64d938acaab",
            "7b0334d187e14b51b700ef375dddd729",
            "2ff410488cdd4bf79e458b92b8f9675d",
            "886a360410d14556b842d9a1c43acaf6",
            "a6305c5bbd2649018899db59b082797c",
            "35e14655566f470cbe4b25f0073ac002"
          ]
        },
        "id": "3jPst7RhsH2R",
        "outputId": "13e2fb2c-08a7-443d-f640-bf387ea6d2cc"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7f6010ac490a42a09d9c764cd126d813"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>█▄▃▄▃▃▃▂▂▂▂▂▁▂▂▁▂▂▂▁▁▁▂▂▂▂▂▂▂▁</td></tr><tr><td>auprc</td><td>▁▅▅▅▄▅▆▆▇▇▇▇▇▇███▇██▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>auroc</td><td>▁▅▄▄▄▅▆▆▇▇▇▆▇▇███▇██▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>avg_train_loss</td><td>█▆▅▆▅▄▅▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>avg_val_loss</td><td>█▃▃▃▃▂▂▂▂▁▁▁▂▁▂▁▂▂▁▁▁▁▂▂▂▂▂▂▂▂</td></tr><tr><td>sensitivity</td><td>▃▅▁▄▂▃▂▅▄▅▂▃▄█▂▂▃▄▄▄▃▃▂▃▂▃▃▅▂▃</td></tr><tr><td>specificity</td><td>▆▇█▇▇▇█▄█▇██▆▁█████████████▆██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.0</td></tr><tr><td>auprc</td><td>0.9831</td></tr><tr><td>auroc</td><td>0.97773</td></tr><tr><td>avg_train_loss</td><td>0.02176</td></tr><tr><td>avg_val_loss</td><td>0.23098</td></tr><tr><td>sensitivity</td><td>0.86957</td></tr><tr><td>specificity</td><td>1.0</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">frosty-field-681</strong> at: <a href='https://wandb.ai/miccai-challenge-2024/task1/runs/31zh5cnm' target=\"_blank\">https://wandb.ai/miccai-challenge-2024/task1/runs/31zh5cnm</a><br/> View project at: <a href='https://wandb.ai/miccai-challenge-2024/task1' target=\"_blank\">https://wandb.ai/miccai-challenge-2024/task1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240720_131833-31zh5cnm/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "source": [
        "if use_wandb:\n",
        "    wandb.finish()  # finish the run"
      ]
    },
    {
      "metadata": {
        "id": "Qbq6om_qXUnS"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7f6010ac490a42a09d9c764cd126d813": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a18f77ae2eb48fc89924cc7660e4143",
              "IPY_MODEL_14501cc83baf447bb325e64d938acaab"
            ],
            "layout": "IPY_MODEL_7b0334d187e14b51b700ef375dddd729"
          }
        },
        "3a18f77ae2eb48fc89924cc7660e4143": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ff410488cdd4bf79e458b92b8f9675d",
            "placeholder": "​",
            "style": "IPY_MODEL_886a360410d14556b842d9a1c43acaf6",
            "value": "0.020 MB of 0.020 MB uploaded\r"
          }
        },
        "14501cc83baf447bb325e64d938acaab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6305c5bbd2649018899db59b082797c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_35e14655566f470cbe4b25f0073ac002",
            "value": 1
          }
        },
        "7b0334d187e14b51b700ef375dddd729": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ff410488cdd4bf79e458b92b8f9675d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "886a360410d14556b842d9a1c43acaf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6305c5bbd2649018899db59b082797c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35e14655566f470cbe4b25f0073ac002": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c60c2c93972b4539a848bcdaa5c4efea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5cfd259607614f5a83afab3ca9929c02",
              "IPY_MODEL_122cce0e48b441839084fca48492e009"
            ],
            "layout": "IPY_MODEL_f8acc357af1d4242a6bcfc63ef595459"
          }
        },
        "5cfd259607614f5a83afab3ca9929c02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d21b6932e0e5452fa8bcd043987ef1a5",
            "placeholder": "​",
            "style": "IPY_MODEL_d975aa6ea38340fca4f5108d51a8905a",
            "value": "0.023 MB of 0.023 MB uploaded\r"
          }
        },
        "122cce0e48b441839084fca48492e009": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fea7c949b8b451eb57337498c111b08",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e4f694aa6435427ab89584177cf52fc3",
            "value": 1
          }
        },
        "f8acc357af1d4242a6bcfc63ef595459": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d21b6932e0e5452fa8bcd043987ef1a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d975aa6ea38340fca4f5108d51a8905a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4fea7c949b8b451eb57337498c111b08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4f694aa6435427ab89584177cf52fc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}