{
 "cells": [
  {
   "metadata": {
    "id": "ehS5McPJTqO7"
   },
   "cell_type": "markdown",
   "source": [
    "### Train for MICCAI challenge on colab using data on gDrive"
   ]
  },
  {
   "metadata": {
    "id": "v92Q3jB2TqO8",
    "outputId": "022e7ae8-9f0a-48bb-aa1f-0f56478960bf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
      "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
      "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
      "Hit:6 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
      "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
      "Get:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
      "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
      "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "Get:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,127 kB]\n",
      "Get:12 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,601 kB]\n",
      "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,998 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,410 kB]\n",
      "Get:15 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [48.1 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,263 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,674 kB]\n",
      "Fetched 12.4 MB in 5s (2,527 kB/s)\n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "git is already the newest version (1:2.34.1-1ubuntu1.11).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n",
      "Collecting loguru\n",
      "  Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m62.5/62.5 kB\u001B[0m \u001B[31m2.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: loguru\n",
      "Successfully installed loguru-0.7.2\n",
      "Collecting efficientnet_pytorch\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet_pytorch) (2.3.0+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (1.12.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (2023.6.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->efficientnet_pytorch)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->efficientnet_pytorch)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->efficientnet_pytorch)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->efficientnet_pytorch)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->efficientnet_pytorch)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->efficientnet_pytorch)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->efficientnet_pytorch)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->efficientnet_pytorch)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->efficientnet_pytorch)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch->efficientnet_pytorch)\n",
      "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->efficientnet_pytorch)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (2.3.0)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->efficientnet_pytorch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m21.3/21.3 MB\u001B[0m \u001B[31m59.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet_pytorch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet_pytorch) (1.3.0)\n",
      "Building wheels for collected packages: efficientnet_pytorch\n",
      "  Building wheel for efficientnet_pytorch (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16429 sha256=201bc53334ccfbf3d5202975557509e6edad38e351d96d20e4655f74aa6374c9\n",
      "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n",
      "Successfully built efficientnet_pytorch\n",
      "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, efficientnet_pytorch\n",
      "Successfully installed efficientnet_pytorch-0.7.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.17.4-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.9 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.9/6.9 MB\u001B[0m \u001B[31m68.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m207.3/207.3 kB\u001B[0m \u001B[31m27.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.9.0-py2.py3-none-any.whl (301 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m301.8/301.8 kB\u001B[0m \u001B[31m33.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m62.7/62.7 kB\u001B[0m \u001B[31m9.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.6.2)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
      "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.9.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.4\n"
     ]
    }
   ],
   "execution_count": 1,
   "source": [
    "# setup\n",
    "!apt-get update\n",
    "!apt-get install git\n",
    "!pip install python-dotenv\n",
    "!pip install loguru\n",
    "!pip install efficientnet_pytorch\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# clone repo in order to have modules available\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "# Define the parameters\n",
    "username = \"bscheuringer\"\n",
    "access_token = \"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\"\n",
    "repo_name = \"AILS-MICCAI-UWF4DR-Challenge\"\n",
    "repo_clone_url = f\"https://{username}:{access_token}@github.com/moritsih/{repo_name}.git\"\n",
    "repo_path = f'/content/{repo_name}'\n",
    "\n",
    "# Check if the repository already exists\n",
    "if not os.path.isdir(repo_path):\n",
    "    !git clone {repo_clone_url}\n",
    "else:\n",
    "    print(\"Repository already exists.\")\n",
    "\n",
    "# navigate to repo directory in order to have working imports\n",
    "%cd {repo_path}\n",
    "\n",
    "!git checkout bsc_colab  # TODO remove when branch is not needed anymore\n",
    "\n",
    "# add repo path to sys path\n",
    "if repo_path not in sys.path:\n",
    "    sys.path.append(repo_path)\n",
    "\n",
    "# Print sys.path to verify\n",
    "print(\"Python Path:\", sys.path)"
   ],
   "metadata": {
    "id": "1Np1qcT1gHgs",
    "outputId": "ecda1b20-705e-49ce-8d6f-2e95ffc4d09c",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cloning into 'AILS-MICCAI-UWF4DR-Challenge'...\n",
      "remote: Enumerating objects: 615, done.\u001B[K\n",
      "remote: Counting objects: 100% (313/313), done.\u001B[K\n",
      "remote: Compressing objects: 100% (192/192), done.\u001B[K\n",
      "remote: Total 615 (delta 188), reused 215 (delta 104), pack-reused 302\u001B[K\n",
      "Receiving objects: 100% (615/615), 82.55 MiB | 15.63 MiB/s, done.\n",
      "Resolving deltas: 100% (360/360), done.\n",
      "/content/AILS-MICCAI-UWF4DR-Challenge\n",
      "Branch 'bsc_colab' set up to track remote branch 'bsc_colab' from 'origin'.\n",
      "Switched to a new branch 'bsc_colab'\n",
      "Python Path: ['/content', '/env/python', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.10/dist-packages/IPython/extensions', '/root/.ipython', '/content/AILS-MICCAI-UWF4DR-Challenge']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# load data and unzip data\n",
    "!python ./tools/download_data_and_chkpts.py"
   ],
   "metadata": {
    "id": "g6Z1v3t65Orz",
    "outputId": "f52b71cc-580f-4fda-f80a-67cb31cfd2fe",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading from 'https://drive.google.com/uc?id=1jm48RSCctyxtEkppS45Znh0wtdf9patA' to 'data/downloads/DeepDRiD.zip.enc'\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1jm48RSCctyxtEkppS45Znh0wtdf9patA\n",
      "From (redirected): https://drive.google.com/uc?id=1jm48RSCctyxtEkppS45Znh0wtdf9patA&confirm=t&uuid=8067b8a1-75a6-41cc-9dc1-b043b357aa4e\n",
      "To: /content/AILS-MICCAI-UWF4DR-Challenge/data/downloads/DeepDRiD.zip.enc\n",
      "100% 303M/303M [00:11<00:00, 27.1MB/s]\n",
      "Downloaded to 'data/downloads/DeepDRiD.zip.enc'\n",
      "Decrypted from 'data/downloads/DeepDRiD.zip.enc' to 'data/downloads/DeepDRiD.zip'\n",
      "Extracting 'data/downloads/DeepDRiD.zip' to 'data/external'\n",
      "Downloading from 'https://drive.google.com/uc?id=1K8xwscXQQo0KXEzFaC2wybgD-UYNXvfc' to 'data/downloads/UWF4DRChallengeData.zip.enc'\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1K8xwscXQQo0KXEzFaC2wybgD-UYNXvfc\n",
      "From (redirected): https://drive.google.com/uc?id=1K8xwscXQQo0KXEzFaC2wybgD-UYNXvfc&confirm=t&uuid=7a824251-69e2-4d54-a825-1f93becc8bc1\n",
      "To: /content/AILS-MICCAI-UWF4DR-Challenge/data/downloads/UWF4DRChallengeData.zip.enc\n",
      "100% 150M/150M [00:04<00:00, 32.4MB/s]\n",
      "Downloaded to 'data/downloads/UWF4DRChallengeData.zip.enc'\n",
      "Decrypted from 'data/downloads/UWF4DRChallengeData.zip.enc' to 'data/downloads/UWF4DRChallengeData.zip'\n",
      "Extracting 'data/downloads/UWF4DRChallengeData.zip' to 'data/raw'\n",
      "Downloading from 'https://drive.google.com/uc?id=1t7Dt8ViDAZ4fLYsFBWmU12Smv10hJyLo' to 'data/downloads/automorph_best_loss_checkpoint.pth'\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1t7Dt8ViDAZ4fLYsFBWmU12Smv10hJyLo\n",
      "From (redirected): https://drive.google.com/uc?id=1t7Dt8ViDAZ4fLYsFBWmU12Smv10hJyLo&confirm=t&uuid=4f9ad096-9fa0-43e7-a825-cfd80f64bee9\n",
      "To: /content/AILS-MICCAI-UWF4DR-Challenge/data/downloads/automorph_best_loss_checkpoint.pth\n",
      "100% 72.9M/72.9M [00:02<00:00, 35.7MB/s]\n",
      "Downloaded to 'data/downloads/automorph_best_loss_checkpoint.pth'\n",
      "File 'data/downloads/automorph_best_loss_checkpoint.pth' is not encrypted. Skipping decryption.\n",
      "Skipping extraction for 'data/downloads/automorph_best_loss_checkpoint.pth'\n",
      "Moving 'data/downloads/automorph_best_loss_checkpoint.pth' to 'models/AutoMorph'\n",
      "Moved to 'models/AutoMorph/automorph_best_loss_checkpoint.pth'\n",
      "Downloading from 'https://drive.google.com/uc?id=1X5gekt4_BbIZLoj2fVMHwjWRRGWotzm1' to 'data/downloads/model_weights.pth'\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1X5gekt4_BbIZLoj2fVMHwjWRRGWotzm1\n",
      "From (redirected): https://drive.google.com/uc?id=1X5gekt4_BbIZLoj2fVMHwjWRRGWotzm1&confirm=t&uuid=e6b7889a-9654-44ac-92fc-df7c78ddb20b\n",
      "To: /content/AILS-MICCAI-UWF4DR-Challenge/data/downloads/model_weights.pth\n",
      "100% 85.3M/85.3M [00:00<00:00, 86.4MB/s]\n",
      "Downloaded to 'data/downloads/model_weights.pth'\n",
      "File 'data/downloads/model_weights.pth' is not encrypted. Skipping decryption.\n",
      "Skipping extraction for 'data/downloads/model_weights.pth'\n",
      "File 'models/submission_eval_model_weights/model_weights.pth' already exists. Skipping move.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# test repo import\n",
    "!ls {repo_path}\n",
    "\n",
    "# try importing a custom class\n",
    "try:\n",
    "    from ails_miccai_uwf4dr_challenge.dataset import DatasetBuilder, CustomDataset\n",
    "\n",
    "    print(\"Import successful!\")\n",
    "except ImportError as e:\n",
    "    print(\"Import failed:\", e)"
   ],
   "metadata": {
    "id": "j-0jnXXgnArd",
    "outputId": "8aba696c-a68b-45b9-eea7-525e3dd18557",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "aes256.key\t\t      docs\tnotebooks\treferences\t  tests\n",
      "ails_miccai_uwf4dr_challenge  Makefile\tpyproject.toml\treports\t\t  tools\n",
      "data\t\t\t      models\tREADME.md\trequirements.txt  wandb\n",
      "Import successful!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Gy3zF2c9TqO9",
    "ExecuteTime": {
     "end_time": "2024-07-12T09:07:17.283441Z",
     "start_time": "2024-07-12T09:07:17.270968Z"
    }
   },
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "from ails_miccai_uwf4dr_challenge.models.metrics import sensitivity_score, specificity_score\n",
    "from ails_miccai_uwf4dr_challenge.models.trainer import Metric, DefaultMetricsEvaluationStrategy, Trainer, TrainingContext, MetricCalculatedHook, PersistBestModelOnEpochEndHook\n",
    "from ails_miccai_uwf4dr_challenge.dataset import DatasetBuilder, DatasetOriginationType, ChallengeTaskType, CustomDataset"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "id": "muTlnyjwTqO9",
    "outputId": "b88a3d77-7637-4aaf-f087-e09cf7d05a47",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "execution_count": null,
   "source": [
    "# connect to gDrive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "ails_data_base_path = Path(\"/content/drive/shared-with-me/data\")\n",
    "my_data_base_path = Path(\"/content/drive/My Drive/JKU/AILS_CHALLENGE_2024\")"
   ]
  },
  {
   "metadata": {
    "id": "t0wClbAUTqO-",
    "outputId": "aa2cc3e2-4a15-410b-f058-848bce1eb1ff",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-07-12T09:04:33.038432Z",
     "start_time": "2024-07-12T09:04:33.021420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# select device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \" + str(device))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T09:13:01.530256Z",
     "start_time": "2024-07-12T09:13:01.516818Z"
    },
    "id": "JKxtdcnuhesI"
   },
   "cell_type": "code",
   "source": [
    "# login to wandb\n",
    "use_wandb = True\n",
    "if use_wandb:\n",
    "    import wandb\n",
    "    #wandb.login()"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "id": "v3cpVuVrTqO-",
    "ExecuteTime": {
     "end_time": "2024-07-12T09:13:08.578358Z",
     "start_time": "2024-07-12T09:13:07.673227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# setup dataset\n",
    "#original_data_dir = ails_data_base_path / \"raw\"\n",
    "#external_data_dir = ails_data_base_path / \"external\"\n",
    "#dataset_builder = DatasetBuilder(dataset='all', task='task1', original_data_dir=original_data_dir, external_data_dir=external_data_dir).get_train_val()\n",
    "dataset_builder = DatasetBuilder(DatasetOriginationType.ALL, ChallengeTaskType.TASK1)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "id": "ZJGshtZshesJ"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 9,
   "source": [
    "from torchvision.transforms import v2\n",
    "import torch\n",
    "\n",
    "# use this augmentation pipeline in the case of:\n",
    "# 1. training\n",
    "# 2. both datasets are included (therefore: resizing or cropping)\n",
    "augment_for_task_1_training = v2.Compose([\n",
    "    v2.ToPILImage(),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    #v2.ColorJitter(brightness=0.5, contrast=0.4, saturation=0.3, hue=0.3),\n",
    "    #v2.RandomHorizontalFlip(),\n",
    "    v2.RandomVerticalFlip(),\n",
    "    v2.RandomRotation(degrees=15, expand=True),\n",
    "    #v2.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    v2.Resize(size=(800, 1016), antialias=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "augment_for_task_1_validation = v2.Compose([\n",
    "    v2.ToPILImage(),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    #v2.ColorJitter(brightness=0.5, contrast=0.4, saturation=0.3, hue=0.3),\n",
    "    #v2.RandomHorizontalFlip(),\n",
    "    #v2.RandomVerticalFlip(),\n",
    "    #v2.RandomRotation(degrees=15, expand=True),\n",
    "    #v2.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    v2.Resize(size=(800, 1016), antialias=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# EfficientNet B0\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "class Task1EfficientNetB0(nn.Module):\n",
    "    def __init__(self, learning_rate=1e-3):\n",
    "        super(Task1EfficientNetB0, self).__init__()\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Get model and replace the last layer\n",
    "        self.model = EfficientNet.from_pretrained('efficientnet-b0', num_classes=1)\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Freeze all layers except the last one\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze the last layer\n",
    "        for param in self.model._fc.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            pred = torch.sigmoid(self(x))\n",
    "        return pred"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Automorph \n",
    "class AutoMorphModel(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(AutoMorphModel, self).__init__()\n",
    "\n",
    "        # code taken from https://github.com/rmaphoh/AutoMorph/blob/main/M1_Retinal_Image_quality_EyePACS/model.py\n",
    "        self.model = EfficientNet.from_pretrained('efficientnet-b4')\n",
    "        self.model._fc = nn.Identity()\n",
    "        net_fl = nn.Sequential(\n",
    "            nn.Linear(1792, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(64, 3)\n",
    "        )\n",
    "        self.model._fc = net_fl\n",
    "        if pretrained:\n",
    "            checkpoint_path = Path().resolve() / \"models\" / \"AutoMorph_Task_1\" / \"automorph_best_loss_checkpoint.pth\"\n",
    "            self.model.load_state_dict(torch.load(checkpoint_path, map_location='cpu'))\n",
    "\n",
    "        # add a final layer that outputs single value\n",
    "        self.model._fc.add_module(\"7\", nn.Linear(3, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = Task1EfficientNetB0()\n",
    "\n",
    "#model = AutoMorphModel(pretrained=True)\n",
    "\n",
    "model.to(device)\n",
    "model_name = model.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YkuNXL9zTqO-",
    "outputId": "12f38601-36c8-45e7-da97-542765916b5e",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 905
    },
    "ExecuteTime": {
     "end_time": "2024-07-12T09:17:48.310776Z",
     "start_time": "2024-07-12T09:13:28.680021Z"
    }
   },
   "source": [
    "# train EfficientNet for image quality assessment (Task 1)\n",
    "print(\"Training model: \",model_name)\n",
    "\n",
    "metrics = [\n",
    "        Metric('auroc', roc_auc_score),\n",
    "        Metric('auprc', average_precision_score),\n",
    "        Metric('accuracy', lambda y_true, y_pred: (y_pred.round() == y_true).mean()),\n",
    "        Metric('sensitivity', sensitivity_score),\n",
    "        Metric('specificity', specificity_score)\n",
    "    ]\n",
    "\n",
    "class WandbLoggingHook(MetricCalculatedHook):\n",
    "        def on_metric_calculated(self, training_context: TrainingContext, metric: Metric, result, last_metric_for_epoch: bool):\n",
    "            import wandb\n",
    "            wandb.log(data={metric.name: result}, commit=last_metric_for_epoch)\n",
    "\n",
    "metrics_eval_strategy = DefaultMetricsEvaluationStrategy(metrics)\n",
    "if(use_wandb):\n",
    "    metrics_eval_strategy.register_metric_calculated_hook(WandbLoggingHook())\n",
    "\n",
    "config = {\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"dataset\": \"UWF4DR-Original\",\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 4,\n",
    "    \"model_type\": model.__class__.__name__\n",
    "}\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config[\"learning_rate\"])\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "train_data, val_data = dataset_builder.get_train_val()\n",
    "train_dataset = CustomDataset(train_data, transform=augment_for_task_1_training)\n",
    "val_dataset = CustomDataset(val_data, transform=augment_for_task_1_validation)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=4)\n",
    "\n",
    "trainer = Trainer(model, train_loader, val_loader, criterion, optimizer, lr_scheduler, device,\n",
    "                        metrics_eval_strategy=metrics_eval_strategy)\n",
    "\n",
    "# build a file name for the model weights containing current timestamp and the model class\n",
    "training_timestamp = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "persist_model_hook = PersistBestModelOnEpochEndHook(my_data_base_path / f\"{model_name}_best_weights_{training_timestamp}.pth\")\n",
    "trainer.add_epoch_end_hook(persist_model_hook) "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b0-355c32eb.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b0-355c32eb.pth\n",
      "100%|██████████| 20.4M/20.4M [00:00<00:00, 368MB/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "Training model:  Task1EfficientNetB0\n",
      "Dataset length:  508\n",
      "Dataset length:  128\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ··········\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/content/AILS-MICCAI-UWF4DR-Challenge/wandb/run-20240712_104530-kqk62q7k</code>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/miccai-challenge-2024/task1/runs/kqk62q7k' target=\"_blank\">helpful-leaf-255</a></strong> to <a href='https://wandb.ai/miccai-challenge-2024/task1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/miccai-challenge-2024/task1' target=\"_blank\">https://wandb.ai/miccai-challenge-2024/task1</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/miccai-challenge-2024/task1/runs/kqk62q7k' target=\"_blank\">https://wandb.ai/miccai-challenge-2024/task1/runs/kqk62q7k</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start training [Task1EfficientNetB0] on [UWF4DR-Original] dataset for [10] epochs with batch size [2]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 1/10 - Avg train Loss: 0.621021: 100%|██████████| 254/254 [05:46<00:00,  1.36s/it]\n",
      "Epoch 1/10 - Avg val Loss: 0.516591: 100%|██████████| 64/64 [00:31<00:00,  2.06it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10 Summary : Train Loss: 0.6210, Val Loss: 0.5166, LR: 1.00e-03, auroc: 0.8918, auprc: 0.8112, accuracy: 0.2266, sensitivity: 0.9348, specificity: 0.7195, avg_train_loss: 0.6210, avg_val_loss: 0.5166\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 2/10 - Avg train Loss: 0.561206: 100%|██████████| 254/254 [05:18<00:00,  1.26s/it]\n",
      "Epoch 2/10 - Avg val Loss: 0.442780: 100%|██████████| 64/64 [00:33<00:00,  1.93it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 2/10 Summary : Train Loss: 0.5612, Val Loss: 0.4428, LR: 1.00e-03, auroc: 0.9403, auprc: 0.9023, accuracy: 0.4062, sensitivity: 0.9130, specificity: 0.8415, avg_train_loss: 0.5612, avg_val_loss: 0.4428\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 3/10 - Avg train Loss: 0.555354: 100%|██████████| 254/254 [05:17<00:00,  1.25s/it]\n",
      "Epoch 3/10 - Avg val Loss: 0.401580: 100%|██████████| 64/64 [00:31<00:00,  2.03it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 3/10 Summary : Train Loss: 0.5554, Val Loss: 0.4016, LR: 1.00e-03, auroc: 0.9515, auprc: 0.9246, accuracy: 0.3203, sensitivity: 0.8261, specificity: 0.9146, avg_train_loss: 0.5554, avg_val_loss: 0.4016\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 4/10 - Avg train Loss: 0.548547: 100%|██████████| 254/254 [05:33<00:00,  1.31s/it]\n",
      "Epoch 4/10 - Avg val Loss: 0.385835: 100%|██████████| 64/64 [00:30<00:00,  2.08it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 4/10 Summary : Train Loss: 0.5485, Val Loss: 0.3858, LR: 1.00e-03, auroc: 0.9539, auprc: 0.9279, accuracy: 0.3594, sensitivity: 0.9130, specificity: 0.8659, avg_train_loss: 0.5485, avg_val_loss: 0.3858\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 5/10 - Avg train Loss: 0.520775: 100%|██████████| 254/254 [05:15<00:00,  1.24s/it]\n",
      "Epoch 5/10 - Avg val Loss: 0.375540: 100%|██████████| 64/64 [00:30<00:00,  2.11it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 5/10 Summary : Train Loss: 0.5208, Val Loss: 0.3755, LR: 1.00e-03, auroc: 0.9549, auprc: 0.9318, accuracy: 0.3516, sensitivity: 0.8696, specificity: 0.9146, avg_train_loss: 0.5208, avg_val_loss: 0.3755\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 6/10 - Avg train Loss: 0.497693: 100%|██████████| 254/254 [05:15<00:00,  1.24s/it]\n",
      "Epoch 6/10 - Avg val Loss: 0.382952: 100%|██████████| 64/64 [00:31<00:00,  2.05it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 6/10 Summary : Train Loss: 0.4977, Val Loss: 0.3830, LR: 1.00e-03, auroc: 0.9496, auprc: 0.9273, accuracy: 0.3750, sensitivity: 0.8913, specificity: 0.9146, avg_train_loss: 0.4977, avg_val_loss: 0.3830\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 7/10 - Avg train Loss: 0.503672: 100%|██████████| 254/254 [05:15<00:00,  1.24s/it]\n",
      "Epoch 7/10 - Avg val Loss: 0.349082: 100%|██████████| 64/64 [00:30<00:00,  2.11it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 7/10 Summary : Train Loss: 0.5037, Val Loss: 0.3491, LR: 1.00e-03, auroc: 0.9544, auprc: 0.9317, accuracy: 0.3281, sensitivity: 0.9130, specificity: 0.8780, avg_train_loss: 0.5037, avg_val_loss: 0.3491\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 8/10 - Avg train Loss: 0.499607: 100%|██████████| 254/254 [05:18<00:00,  1.25s/it]\n",
      "Epoch 8/10 - Avg val Loss: 0.340515: 100%|██████████| 64/64 [00:31<00:00,  2.06it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 8/10 Summary : Train Loss: 0.4996, Val Loss: 0.3405, LR: 1.00e-03, auroc: 0.9547, auprc: 0.9308, accuracy: 0.3516, sensitivity: 0.8696, specificity: 0.9146, avg_train_loss: 0.4996, avg_val_loss: 0.3405\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 9/10 - Avg train Loss: 0.505307: 100%|██████████| 254/254 [05:16<00:00,  1.25s/it]\n",
      "Epoch 9/10 - Avg val Loss: 0.355670: 100%|██████████| 64/64 [00:31<00:00,  2.06it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 9/10 Summary : Train Loss: 0.5053, Val Loss: 0.3557, LR: 1.00e-03, auroc: 0.9502, auprc: 0.9265, accuracy: 0.3594, sensitivity: 0.8913, specificity: 0.8902, avg_train_loss: 0.5053, avg_val_loss: 0.3557\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 10/10 - Avg train Loss: 0.508065: 100%|██████████| 254/254 [05:20<00:00,  1.26s/it]\n",
      "Epoch 10/10 - Avg val Loss: 0.336713: 100%|██████████| 64/64 [00:30<00:00,  2.10it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 10/10 Summary : Train Loss: 0.5081, Val Loss: 0.3367, LR: 1.00e-03, auroc: 0.9512, auprc: 0.9234, accuracy: 0.3438, sensitivity: 0.9130, specificity: 0.8659, avg_train_loss: 0.5081, avg_val_loss: 0.3367\n",
      "Training finished.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if use_wandb:\n",
    "    wandb.init(entity='miccai-challenge-2024' ,project='task1', config=config)\n",
    "\n",
    "print(f\"Start training [{config['model_type']}] on [{config['dataset']}] dataset for [{config['epochs']}] epochs with batch size [{config['batch_size']}]\")\n",
    "\n",
    "trainer.train(num_epochs=config[\"epochs\"])\n",
    "\n",
    "print(\"Training finished.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
