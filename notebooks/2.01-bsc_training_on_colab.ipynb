{
  "cells": [
    {
      "metadata": {
        "id": "ehS5McPJTqO7"
      },
      "cell_type": "markdown",
      "source": [
        "### Train for MICCAI challenge on colab using data on gDrive"
      ]
    },
    {
      "metadata": {
        "id": "v92Q3jB2TqO8",
        "outputId": "5c17453c-6930-4961-e96d-918931ce661a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.81)] [1 InRelease 3,626 B/3\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.81)] [Connecting to ppa.lau\r                                                                                                    \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,263 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,410 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,674 kB]\n",
            "Get:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,998 kB]\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:15 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [48.1 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,127 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,601 kB]\n",
            "Fetched 12.4 MB in 3s (4,751 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.11).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n",
            "Collecting loguru\n",
            "  Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: loguru\n",
            "Successfully installed loguru-0.7.2\n",
            "Collecting efficientnet_pytorch\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet_pytorch) (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->efficientnet_pytorch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->efficientnet_pytorch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet_pytorch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet_pytorch) (1.3.0)\n",
            "Building wheels for collected packages: efficientnet_pytorch\n",
            "  Building wheel for efficientnet_pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16429 sha256=9e38756e428d7b53acf6ef141321a3e71dce867d7b0dc3a4a17a82a04df2c945\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n",
            "Successfully built efficientnet_pytorch\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, efficientnet_pytorch\n",
            "Successfully installed efficientnet_pytorch-0.7.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "execution_count": 1,
      "source": [
        "# setup\n",
        "!apt-get update\n",
        "!apt-get install git\n",
        "!pip install python-dotenv\n",
        "!pip install loguru\n",
        "!pip install efficientnet_pytorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clone repo in order to have modules available\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "# Define the parameters\n",
        "username = \"bscheuringer\"\n",
        "access_token = \"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\"\n",
        "repo_name = \"AILS-MICCAI-UWF4DR-Challenge\"\n",
        "repo_clone_url = f\"https://{username}:{access_token}@github.com/moritsih/{repo_name}.git\"\n",
        "repo_path = f'/content/{repo_name}'\n",
        "\n",
        "# Check if the repository already exists\n",
        "if not os.path.isdir(repo_path):\n",
        "    !git clone {repo_clone_url}\n",
        "else:\n",
        "    print(\"Repository already exists.\")\n",
        "\n",
        "# navigate to repo directory in order to have working imports\n",
        "%cd {repo_path}\n",
        "\n",
        "!git checkout bsc_colab  # TODO remove when branch is not needed anymore\n",
        "\n",
        "# add repo path to sys path\n",
        "if repo_path not in sys.path:\n",
        "    sys.path.append(repo_path)\n",
        "\n",
        "# Print sys.path to verify\n",
        "print(\"Python Path:\", sys.path)"
      ],
      "metadata": {
        "id": "1Np1qcT1gHgs",
        "outputId": "0b0c1b1d-ef4d-4457-94d4-a098d5a88b14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AILS-MICCAI-UWF4DR-Challenge'...\n",
            "remote: Enumerating objects: 579, done.\u001b[K\n",
            "remote: Counting objects: 100% (277/277), done.\u001b[K\n",
            "remote: Compressing objects: 100% (170/170), done.\u001b[K\n",
            "remote: Total 579 (delta 165), reused 190 (delta 93), pack-reused 302\u001b[K\n",
            "Receiving objects: 100% (579/579), 82.54 MiB | 34.47 MiB/s, done.\n",
            "Resolving deltas: 100% (337/337), done.\n",
            "/content/AILS-MICCAI-UWF4DR-Challenge\n",
            "Branch 'bsc_colab' set up to track remote branch 'bsc_colab' from 'origin'.\n",
            "Switched to a new branch 'bsc_colab'\n",
            "Python Path: ['/content', '/env/python', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.10/dist-packages/IPython/extensions', '/root/.ipython', '/content/AILS-MICCAI-UWF4DR-Challenge']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load data and unzip data\n",
        "!python ./tools/download_data_and_chkpts.py"
      ],
      "metadata": {
        "id": "g6Z1v3t65Orz",
        "outputId": "08cd92b8-dbaf-4c5e-d8fa-34721940e091",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from 'https://drive.google.com/uc?id=1jm48RSCctyxtEkppS45Znh0wtdf9patA' to 'data/downloads/DeepDRiD.zip.enc'\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1jm48RSCctyxtEkppS45Znh0wtdf9patA\n",
            "From (redirected): https://drive.google.com/uc?id=1jm48RSCctyxtEkppS45Znh0wtdf9patA&confirm=t&uuid=096a8a5e-bd21-4dd9-b7df-5fa0fc55a9fe\n",
            "To: /content/AILS-MICCAI-UWF4DR-Challenge/data/downloads/DeepDRiD.zip.enc\n",
            "100% 303M/303M [00:07<00:00, 41.9MB/s]\n",
            "Downloaded to 'data/downloads/DeepDRiD.zip.enc'\n",
            "Decrypted from 'data/downloads/DeepDRiD.zip.enc' to 'data/downloads/DeepDRiD.zip'\n",
            "Extracting 'data/downloads/DeepDRiD.zip' to 'data/external'\n",
            "Downloading from 'https://drive.google.com/uc?id=1K8xwscXQQo0KXEzFaC2wybgD-UYNXvfc' to 'data/downloads/UWF4DRChallengeData.zip.enc'\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1K8xwscXQQo0KXEzFaC2wybgD-UYNXvfc\n",
            "From (redirected): https://drive.google.com/uc?id=1K8xwscXQQo0KXEzFaC2wybgD-UYNXvfc&confirm=t&uuid=88fd3ab9-af93-4eaa-8869-c9b7d4279072\n",
            "To: /content/AILS-MICCAI-UWF4DR-Challenge/data/downloads/UWF4DRChallengeData.zip.enc\n",
            "100% 150M/150M [00:01<00:00, 145MB/s]\n",
            "Downloaded to 'data/downloads/UWF4DRChallengeData.zip.enc'\n",
            "Decrypted from 'data/downloads/UWF4DRChallengeData.zip.enc' to 'data/downloads/UWF4DRChallengeData.zip'\n",
            "Extracting 'data/downloads/UWF4DRChallengeData.zip' to 'data/raw'\n",
            "Downloading from 'https://drive.google.com/uc?id=1t7Dt8ViDAZ4fLYsFBWmU12Smv10hJyLo' to 'data/downloads/automorph_best_loss_checkpoint.pth'\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1t7Dt8ViDAZ4fLYsFBWmU12Smv10hJyLo\n",
            "From (redirected): https://drive.google.com/uc?id=1t7Dt8ViDAZ4fLYsFBWmU12Smv10hJyLo&confirm=t&uuid=c0a90a5d-1c28-41bd-bf4f-9e73d38f47a7\n",
            "To: /content/AILS-MICCAI-UWF4DR-Challenge/data/downloads/automorph_best_loss_checkpoint.pth\n",
            "100% 72.9M/72.9M [00:00<00:00, 110MB/s]\n",
            "Downloaded to 'data/downloads/automorph_best_loss_checkpoint.pth'\n",
            "File 'data/downloads/automorph_best_loss_checkpoint.pth' is not encrypted. Skipping decryption.\n",
            "Skipping extraction for 'data/downloads/automorph_best_loss_checkpoint.pth'\n",
            "Moving 'data/downloads/automorph_best_loss_checkpoint.pth' to 'models/AutoMorph'\n",
            "Moved to 'models/AutoMorph/automorph_best_loss_checkpoint.pth'\n",
            "Downloading from 'https://drive.google.com/uc?id=1X5gekt4_BbIZLoj2fVMHwjWRRGWotzm1' to 'data/downloads/model_weights.pth'\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1X5gekt4_BbIZLoj2fVMHwjWRRGWotzm1\n",
            "To: /content/AILS-MICCAI-UWF4DR-Challenge/data/downloads/model_weights.pth\n",
            "100% 85.3M/85.3M [00:01<00:00, 52.9MB/s]\n",
            "Downloaded to 'data/downloads/model_weights.pth'\n",
            "File 'data/downloads/model_weights.pth' is not encrypted. Skipping decryption.\n",
            "Skipping extraction for 'data/downloads/model_weights.pth'\n",
            "File 'models/submission_eval_model_weights/model_weights.pth' already exists. Skipping move.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test repo import\n",
        "!ls {repo_path}\n",
        "\n",
        "# try importing a custom class\n",
        "try:\n",
        "    from ails_miccai_uwf4dr_challenge.dataset import DatasetBuilder, CustomDataset\n",
        "\n",
        "    print(\"Import successful!\")\n",
        "except ImportError as e:\n",
        "    print(\"Import failed:\", e)"
      ],
      "metadata": {
        "id": "j-0jnXXgnArd",
        "outputId": "a096e7a0-7707-4225-8da1-90adf2833a89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aes256.key\t\t      docs\tnotebooks\treferences\t  tests\n",
            "ails_miccai_uwf4dr_challenge  Makefile\tpyproject.toml\treports\t\t  tools\n",
            "data\t\t\t      models\tREADME.md\trequirements.txt  wandb\n",
            "Import successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Gy3zF2c9TqO9"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "from ails_miccai_uwf4dr_challenge.dataset import DatasetBuilder, DatasetOriginationType, ChallengeTaskType\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from ails_miccai_uwf4dr_challenge.augmentations import rotate_affine_flip_choice, resize_only\n",
        "from ails_miccai_uwf4dr_challenge.models.metrics import sensitivity_score, specificity_score\n",
        "from ails_miccai_uwf4dr_challenge.models.trainer import Metric, DefaultMetricsEvaluationStrategy, Trainer"
      ]
    },
    {
      "metadata": {
        "id": "muTlnyjwTqO9",
        "outputId": "b88a3d77-7637-4aaf-f087-e09cf7d05a47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "execution_count": 7,
      "source": [
        "# connect to gDrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "ails_data_base_path = Path(\"/content/drive/shared-with-me/data\")\n",
        "my_data_base_path = Path(\"/content/drive/My Drive/JKU/AILS_CHALLENGE_2024\")"
      ]
    },
    {
      "metadata": {
        "id": "t0wClbAUTqO-",
        "outputId": "488c98b6-22d9-46ad-8c45-d2e184967feb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "execution_count": 8,
      "source": [
        "# select device for training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device: \" + str(device))"
      ]
    },
    {
      "metadata": {
        "id": "v3cpVuVrTqO-"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 6,
      "source": [
        "# setup dataset\n",
        "#original_data_dir = ails_data_base_path / \"raw\"\n",
        "#external_data_dir = ails_data_base_path / \"external\"\n",
        "#dataset_builder = DatasetBuilder(dataset='all', task='task1', original_data_dir=original_data_dir, external_data_dir=external_data_dir).get_train_val()\n",
        "dataset_builder = DatasetBuilder(DatasetOriginationType.ALL, ChallengeTaskType.TASK1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YkuNXL9zTqO-",
        "outputId": "0ea2e4de-fb1f-4019-bdb9-70d78b58d132",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained weights for efficientnet-b4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model:  Task1EfficientNetB4\n",
            "Dataset length:  508\n",
            "Dataset length:  128\n",
            "Start training [Task1EfficientNetB4] on [UWF4DR-Original] dataset for [10] epochs with batch size [2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 - Avg train Loss: 0.621635: 100%|██████████| 254/254 [16:59<00:00,  4.01s/it]\n",
            "Epoch 1/10 - Avg val Loss: 0.766125: 100%|██████████| 64/64 [00:28<00:00,  2.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 Summary : Train Loss: 0.6216, Val Loss: 0.7661, LR: 1.00e-03, auroc: 0.6013, auprc: 0.4441, accuracy: 0.6094, sensitivity: 0.7174, specificity: 0.4878, avg_train_loss: 0.6216, avg_val_loss: 0.7661\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10 - Avg train Loss: 0.545711:  23%|██▎       | 58/254 [03:24<11:30,  3.53s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-fc3141377a0f>\u001b[0m in \u001b[0;36m<cell line: 71>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Start training [{config['model_type']}] on [{config['dataset']}] dataset for [{config['epochs']}] epochs with batch size [{config['batch_size']}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"epochs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training finished.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/AILS-MICCAI-UWF4DR-Challenge/ails_miccai_uwf4dr_challenge/models/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_epochs, num_batches)\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0mtraining_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m             \u001b[0mmodel_train_results\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModelResultsAndLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m             \u001b[0mmodel_val_results\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModelResultsAndLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/AILS-MICCAI-UWF4DR-Challenge/ails_miccai_uwf4dr_challenge/models/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training_context, train_loader)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtraining_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTimings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBATCH_PROCESSING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                     \u001b[0mbatch_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m                     \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_batch_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/AILS-MICCAI-UWF4DR-Challenge/ails_miccai_uwf4dr_challenge/models/trainer.py\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(self, training_context, batch)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mtraining_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mModelResultsAndLabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModelResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midentifiers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDefaultBatchValidationStrategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBatchValidationStrategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# train EfficientNet for image quality assessment (Task 1)\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "class Task1EfficientNetB4(nn.Module):\n",
        "    def __init__(self, learning_rate=1e-3):\n",
        "        super(Task1EfficientNetB4, self).__init__()\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Get model and replace the last layer\n",
        "        self.model = EfficientNet.from_pretrained('efficientnet-b4', num_classes=1)\n",
        "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        # Freeze all layers except the last one\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Unfreeze the last layer\n",
        "        for param in self.model._fc.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def predict(self, x):\n",
        "        with torch.no_grad():\n",
        "            pred = torch.sigmoid(self(x))\n",
        "        return pred\n",
        "\n",
        "\n",
        "model = Task1EfficientNetB4()\n",
        "\n",
        "\n",
        "\n",
        "model.to(device)\n",
        "print(\"Training model: \", model.__class__.__name__)\n",
        "\n",
        "metrics = [\n",
        "        Metric('auroc', roc_auc_score),\n",
        "        Metric('auprc', average_precision_score),\n",
        "        Metric('accuracy', lambda y_true, y_pred: (y_pred.round() == y_true).mean()),\n",
        "        Metric('sensitivity', sensitivity_score),\n",
        "        Metric('specificity', specificity_score)\n",
        "    ]\n",
        "\n",
        "metrics_eval_strategy = DefaultMetricsEvaluationStrategy(metrics)\n",
        "\n",
        "config = {\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"dataset\": \"UWF4DR-Original\",\n",
        "    \"epochs\": 10,\n",
        "    \"batch_size\": 2,\n",
        "    \"model_type\": model.__class__.__name__\n",
        "}\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=config[\"learning_rate\"])\n",
        "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
        "\n",
        "train_data, val_data = dataset_builder.get_train_val()\n",
        "train_dataset = CustomDataset(train_data, transform=rotate_affine_flip_choice)\n",
        "val_dataset = CustomDataset(val_data, transform=resize_only)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
        "\n",
        "trainer = Trainer(model, train_loader, val_loader, criterion, optimizer, lr_scheduler, device,\n",
        "                        metrics_eval_strategy=metrics_eval_strategy)\n",
        "\n",
        "print(f\"Start training [{config['model_type']}] on [{config['dataset']}] dataset for [{config['epochs']}] epochs with batch size [{config['batch_size']}]\")\n",
        "\n",
        "trainer.train(num_epochs=config[\"epochs\"])\n",
        "\n",
        "print(\"Training finished.\")\n",
        "\n",
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}