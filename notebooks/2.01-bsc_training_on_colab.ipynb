{
 "cells": [
  {
   "metadata": {
    "id": "ehS5McPJTqO7"
   },
   "cell_type": "markdown",
   "source": [
    "### Train for MICCAI challenge on colab using data on gDrive"
   ]
  },
  {
   "metadata": {
    "id": "v92Q3jB2TqO8",
    "outputId": "5c17453c-6930-4961-e96d-918931ce661a",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
      "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.81)] [1 InRelease 3,626 B/3\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.81)] [Connecting to ppa.lau\r                                                                                                    \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
      "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
      "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,263 kB]\n",
      "Hit:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
      "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,410 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,674 kB]\n",
      "Get:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
      "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,998 kB]\n",
      "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "Get:15 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [48.1 kB]\n",
      "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,127 kB]\n",
      "Get:17 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,601 kB]\n",
      "Fetched 12.4 MB in 3s (4,751 kB/s)\n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "git is already the newest version (1:2.34.1-1ubuntu1.11).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n",
      "Collecting loguru\n",
      "  Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m62.5/62.5 kB\u001B[0m \u001B[31m2.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: loguru\n",
      "Successfully installed loguru-0.7.2\n",
      "Collecting efficientnet_pytorch\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet_pytorch) (2.3.0+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (1.12.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (2023.6.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->efficientnet_pytorch)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->efficientnet_pytorch)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->efficientnet_pytorch)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->efficientnet_pytorch)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->efficientnet_pytorch)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->efficientnet_pytorch)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->efficientnet_pytorch)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->efficientnet_pytorch)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->efficientnet_pytorch)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch->efficientnet_pytorch)\n",
      "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->efficientnet_pytorch)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (2.3.0)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->efficientnet_pytorch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m21.3/21.3 MB\u001B[0m \u001B[31m59.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet_pytorch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet_pytorch) (1.3.0)\n",
      "Building wheels for collected packages: efficientnet_pytorch\n",
      "  Building wheel for efficientnet_pytorch (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16429 sha256=9e38756e428d7b53acf6ef141321a3e71dce867d7b0dc3a4a17a82a04df2c945\n",
      "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n",
      "Successfully built efficientnet_pytorch\n",
      "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, efficientnet_pytorch\n",
      "Successfully installed efficientnet_pytorch-0.7.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n"
     ]
    }
   ],
   "execution_count": 1,
   "source": [
    "# setup\n",
    "!apt-get update\n",
    "!apt-get install git\n",
    "!pip install python-dotenv\n",
    "!pip install loguru\n",
    "!pip install efficientnet_pytorch\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# clone repo in order to have modules available\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "# Define the parameters\n",
    "username = \"bscheuringer\"\n",
    "access_token = \"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\"\n",
    "repo_name = \"AILS-MICCAI-UWF4DR-Challenge\"\n",
    "repo_clone_url = f\"https://{username}:{access_token}@github.com/moritsih/{repo_name}.git\"\n",
    "repo_path = f'/content/{repo_name}'\n",
    "\n",
    "# Check if the repository already exists\n",
    "if not os.path.isdir(repo_path):\n",
    "    !git clone {repo_clone_url}\n",
    "else:\n",
    "    print(\"Repository already exists.\")\n",
    "\n",
    "# navigate to repo directory in order to have working imports\n",
    "%cd {repo_path}\n",
    "\n",
    "!git checkout bsc_colab  # TODO remove when branch is not needed anymore\n",
    "\n",
    "# add repo path to sys path\n",
    "if repo_path not in sys.path:\n",
    "    sys.path.append(repo_path)\n",
    "\n",
    "# Print sys.path to verify\n",
    "print(\"Python Path:\", sys.path)"
   ],
   "metadata": {
    "id": "1Np1qcT1gHgs",
    "outputId": "0b0c1b1d-ef4d-4457-94d4-a098d5a88b14",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cloning into 'AILS-MICCAI-UWF4DR-Challenge'...\n",
      "remote: Enumerating objects: 579, done.\u001B[K\n",
      "remote: Counting objects: 100% (277/277), done.\u001B[K\n",
      "remote: Compressing objects: 100% (170/170), done.\u001B[K\n",
      "remote: Total 579 (delta 165), reused 190 (delta 93), pack-reused 302\u001B[K\n",
      "Receiving objects: 100% (579/579), 82.54 MiB | 34.47 MiB/s, done.\n",
      "Resolving deltas: 100% (337/337), done.\n",
      "/content/AILS-MICCAI-UWF4DR-Challenge\n",
      "Branch 'bsc_colab' set up to track remote branch 'bsc_colab' from 'origin'.\n",
      "Switched to a new branch 'bsc_colab'\n",
      "Python Path: ['/content', '/env/python', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.10/dist-packages/IPython/extensions', '/root/.ipython', '/content/AILS-MICCAI-UWF4DR-Challenge']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# load data and unzip data\n",
    "!python ./tools/download_data_and_chkpts.py"
   ],
   "metadata": {
    "id": "g6Z1v3t65Orz",
    "outputId": "08cd92b8-dbaf-4c5e-d8fa-34721940e091",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading from 'https://drive.google.com/uc?id=1jm48RSCctyxtEkppS45Znh0wtdf9patA' to 'data/downloads/DeepDRiD.zip.enc'\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1jm48RSCctyxtEkppS45Znh0wtdf9patA\n",
      "From (redirected): https://drive.google.com/uc?id=1jm48RSCctyxtEkppS45Znh0wtdf9patA&confirm=t&uuid=096a8a5e-bd21-4dd9-b7df-5fa0fc55a9fe\n",
      "To: /content/AILS-MICCAI-UWF4DR-Challenge/data/downloads/DeepDRiD.zip.enc\n",
      "100% 303M/303M [00:07<00:00, 41.9MB/s]\n",
      "Downloaded to 'data/downloads/DeepDRiD.zip.enc'\n",
      "Decrypted from 'data/downloads/DeepDRiD.zip.enc' to 'data/downloads/DeepDRiD.zip'\n",
      "Extracting 'data/downloads/DeepDRiD.zip' to 'data/external'\n",
      "Downloading from 'https://drive.google.com/uc?id=1K8xwscXQQo0KXEzFaC2wybgD-UYNXvfc' to 'data/downloads/UWF4DRChallengeData.zip.enc'\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1K8xwscXQQo0KXEzFaC2wybgD-UYNXvfc\n",
      "From (redirected): https://drive.google.com/uc?id=1K8xwscXQQo0KXEzFaC2wybgD-UYNXvfc&confirm=t&uuid=88fd3ab9-af93-4eaa-8869-c9b7d4279072\n",
      "To: /content/AILS-MICCAI-UWF4DR-Challenge/data/downloads/UWF4DRChallengeData.zip.enc\n",
      "100% 150M/150M [00:01<00:00, 145MB/s]\n",
      "Downloaded to 'data/downloads/UWF4DRChallengeData.zip.enc'\n",
      "Decrypted from 'data/downloads/UWF4DRChallengeData.zip.enc' to 'data/downloads/UWF4DRChallengeData.zip'\n",
      "Extracting 'data/downloads/UWF4DRChallengeData.zip' to 'data/raw'\n",
      "Downloading from 'https://drive.google.com/uc?id=1t7Dt8ViDAZ4fLYsFBWmU12Smv10hJyLo' to 'data/downloads/automorph_best_loss_checkpoint.pth'\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1t7Dt8ViDAZ4fLYsFBWmU12Smv10hJyLo\n",
      "From (redirected): https://drive.google.com/uc?id=1t7Dt8ViDAZ4fLYsFBWmU12Smv10hJyLo&confirm=t&uuid=c0a90a5d-1c28-41bd-bf4f-9e73d38f47a7\n",
      "To: /content/AILS-MICCAI-UWF4DR-Challenge/data/downloads/automorph_best_loss_checkpoint.pth\n",
      "100% 72.9M/72.9M [00:00<00:00, 110MB/s]\n",
      "Downloaded to 'data/downloads/automorph_best_loss_checkpoint.pth'\n",
      "File 'data/downloads/automorph_best_loss_checkpoint.pth' is not encrypted. Skipping decryption.\n",
      "Skipping extraction for 'data/downloads/automorph_best_loss_checkpoint.pth'\n",
      "Moving 'data/downloads/automorph_best_loss_checkpoint.pth' to 'models/AutoMorph'\n",
      "Moved to 'models/AutoMorph/automorph_best_loss_checkpoint.pth'\n",
      "Downloading from 'https://drive.google.com/uc?id=1X5gekt4_BbIZLoj2fVMHwjWRRGWotzm1' to 'data/downloads/model_weights.pth'\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1X5gekt4_BbIZLoj2fVMHwjWRRGWotzm1\n",
      "To: /content/AILS-MICCAI-UWF4DR-Challenge/data/downloads/model_weights.pth\n",
      "100% 85.3M/85.3M [00:01<00:00, 52.9MB/s]\n",
      "Downloaded to 'data/downloads/model_weights.pth'\n",
      "File 'data/downloads/model_weights.pth' is not encrypted. Skipping decryption.\n",
      "Skipping extraction for 'data/downloads/model_weights.pth'\n",
      "File 'models/submission_eval_model_weights/model_weights.pth' already exists. Skipping move.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# test repo import\n",
    "!ls {repo_path}\n",
    "\n",
    "# try importing a custom class\n",
    "try:\n",
    "    from ails_miccai_uwf4dr_challenge.dataset import DatasetBuilder, CustomDataset\n",
    "\n",
    "    print(\"Import successful!\")\n",
    "except ImportError as e:\n",
    "    print(\"Import failed:\", e)"
   ],
   "metadata": {
    "id": "j-0jnXXgnArd",
    "outputId": "a096e7a0-7707-4225-8da1-90adf2833a89",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "aes256.key\t\t      docs\tnotebooks\treferences\t  tests\n",
      "ails_miccai_uwf4dr_challenge  Makefile\tpyproject.toml\treports\t\t  tools\n",
      "data\t\t\t      models\tREADME.md\trequirements.txt  wandb\n",
      "Import successful!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Gy3zF2c9TqO9",
    "ExecuteTime": {
     "end_time": "2024-07-12T09:07:17.283441Z",
     "start_time": "2024-07-12T09:07:17.270968Z"
    }
   },
   "source": [
    "# imports\n",
    "from ails_miccai_uwf4dr_challenge.dataset import DatasetBuilder, DatasetOriginationType, ChallengeTaskType, CustomDataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from ails_miccai_uwf4dr_challenge.augmentations import rotate_affine_flip_choice, resize_only\n",
    "from ails_miccai_uwf4dr_challenge.models.metrics import sensitivity_score, specificity_score\n",
    "from ails_miccai_uwf4dr_challenge.models.trainer import Metric, DefaultMetricsEvaluationStrategy, Trainer, TrainingContext, MetricCalculatedHook"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "id": "muTlnyjwTqO9",
    "outputId": "b88a3d77-7637-4aaf-f087-e09cf7d05a47",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "execution_count": 7,
   "source": [
    "# connect to gDrive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "ails_data_base_path = Path(\"/content/drive/shared-with-me/data\")\n",
    "my_data_base_path = Path(\"/content/drive/My Drive/JKU/AILS_CHALLENGE_2024\")"
   ]
  },
  {
   "metadata": {
    "id": "t0wClbAUTqO-",
    "outputId": "488c98b6-22d9-46ad-8c45-d2e184967feb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-07-12T09:04:33.038432Z",
     "start_time": "2024-07-12T09:04:33.021420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# select device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \" + str(device))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T09:13:01.530256Z",
     "start_time": "2024-07-12T09:13:01.516818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# login to wandb\n",
    "use_wandb = True\n",
    "if use_wandb:\n",
    "    import wandb\n",
    "    wandb.login()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: b-scheuringer (miccai-challenge-2024). Use `wandb login --relogin` to force relogin\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "id": "v3cpVuVrTqO-",
    "ExecuteTime": {
     "end_time": "2024-07-12T09:13:08.578358Z",
     "start_time": "2024-07-12T09:13:07.673227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# setup dataset\n",
    "#original_data_dir = ails_data_base_path / \"raw\"\n",
    "#external_data_dir = ails_data_base_path / \"external\"\n",
    "#dataset_builder = DatasetBuilder(dataset='all', task='task1', original_data_dir=original_data_dir, external_data_dir=external_data_dir).get_train_val()\n",
    "dataset_builder = DatasetBuilder(DatasetOriginationType.ALL, ChallengeTaskType.TASK1)"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torchvision.transforms import v2\n",
    "import torch\n",
    "\n",
    "# use this augmentation pipeline in the case of:\n",
    "# 1. training\n",
    "# 2. both datasets are included (therefore: resizing or cropping)\n",
    "augment_for_task_1_training = v2.Compose([\n",
    "    v2.ToPILImage(),\n",
    "    v2.ToImage(), \n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    #v2.ColorJitter(brightness=0.5, contrast=0.4, saturation=0.3, hue=0.3),\n",
    "    #v2.RandomHorizontalFlip(),\n",
    "    v2.RandomVerticalFlip(),\n",
    "    v2.RandomRotation(degrees=15, expand=True),\n",
    "    #v2.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    v2.Resize(size=(800, 1016), antialias=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "augment_for_task_1_validation = v2.Compose([\n",
    "    v2.ToPILImage(),\n",
    "    v2.ToImage(), \n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    #v2.ColorJitter(brightness=0.5, contrast=0.4, saturation=0.3, hue=0.3),\n",
    "    #v2.RandomHorizontalFlip(),\n",
    "    #v2.RandomVerticalFlip(),\n",
    "    #v2.RandomRotation(degrees=15, expand=True),\n",
    "    #v2.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    v2.Resize(size=(800, 1016), antialias=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YkuNXL9zTqO-",
    "outputId": "0ea2e4de-fb1f-4019-bdb9-70d78b58d132",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 602
    },
    "ExecuteTime": {
     "end_time": "2024-07-12T09:17:48.310776Z",
     "start_time": "2024-07-12T09:13:28.680021Z"
    }
   },
   "source": [
    "# train EfficientNet for image quality assessment (Task 1)\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "class Task1EfficientNetB0(nn.Module):\n",
    "    def __init__(self, learning_rate=1e-3):\n",
    "        super(Task1EfficientNetB0, self).__init__()\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Get model and replace the last layer\n",
    "        self.model = EfficientNet.from_pretrained('efficientnet-b0', num_classes=1)\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Freeze all layers except the last one\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze the last layer\n",
    "        for param in self.model._fc.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            pred = torch.sigmoid(self(x))\n",
    "        return pred\n",
    "\n",
    "\n",
    "model = Task1EfficientNetB0()\n",
    "\n",
    "model.to(device)\n",
    "print(\"Training model: \", model.__class__.__name__)\n",
    "\n",
    "metrics = [\n",
    "        Metric('auroc', roc_auc_score),\n",
    "        Metric('auprc', average_precision_score),\n",
    "        Metric('accuracy', lambda y_true, y_pred: (y_pred.round() == y_true).mean()),\n",
    "        Metric('sensitivity', sensitivity_score),\n",
    "        Metric('specificity', specificity_score)\n",
    "    ]\n",
    "\n",
    "class WandbLoggingHook(MetricCalculatedHook):\n",
    "        def on_metric_calculated(self, training_context: TrainingContext, metric: Metric, result, last_metric_for_epoch: bool):\n",
    "            import wandb\n",
    "            wandb.log(data={metric.name: result}, commit=last_metric_for_epoch)\n",
    "\n",
    "metrics_eval_strategy = DefaultMetricsEvaluationStrategy(metrics)\n",
    "if(use_wandb):\n",
    "    metrics_eval_strategy.register_metric_calculated_hook(WandbLoggingHook())\n",
    "\n",
    "config = {\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"dataset\": \"UWF4DR-Original\",\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 2,\n",
    "    \"model_type\": model.__class__.__name__\n",
    "}\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config[\"learning_rate\"])\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "train_data, val_data = dataset_builder.get_train_val()\n",
    "train_dataset = CustomDataset(train_data, transform=augment_for_task_1_training)\n",
    "val_dataset = CustomDataset(val_data, transform=augment_for_task_1_validation)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "trainer = Trainer(model, train_loader, val_loader, criterion, optimizer, lr_scheduler, device,\n",
    "                        metrics_eval_strategy=metrics_eval_strategy)\n",
    "\n",
    "if use_wandb:\n",
    "    wandb.init(entity='miccai-challenge-2024' ,project='task1', config=config)\n",
    "\n",
    "print(f\"Start training [{config['model_type']}] on [{config['dataset']}] dataset for [{config['epochs']}] epochs with batch size [{config['batch_size']}]\")\n",
    "\n",
    "trainer.train(num_epochs=config[\"epochs\"])\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "Training model:  Task1EfficientNetB0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\berth\\miniconda3\\envs\\AILS-MICCAI-UWF4DR\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length:  508\n",
      "Dataset length:  128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\berth\\0_DEV\\0_JKU\\ails_miccai\\AILS-MICCAI-UWF4DR-Challenge\\notebooks\\wandb\\run-20240712_111328-v5xqy1m5</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/miccai-challenge-2024/task1/runs/v5xqy1m5' target=\"_blank\">ethereal-breeze-254</a></strong> to <a href='https://wandb.ai/miccai-challenge-2024/task1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/miccai-challenge-2024/task1' target=\"_blank\">https://wandb.ai/miccai-challenge-2024/task1</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/miccai-challenge-2024/task1/runs/v5xqy1m5' target=\"_blank\">https://wandb.ai/miccai-challenge-2024/task1/runs/v5xqy1m5</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training [Task1EfficientNetB0] on [UWF4DR-Original] dataset for [10] epochs with batch size [2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Avg train Loss: 0.660337:  11%|█▏        | 29/254 [04:17<33:14,  8.86s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 79\u001B[0m\n\u001B[0;32m     74\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(model, train_loader, val_loader, criterion, optimizer, lr_scheduler, device,\n\u001B[0;32m     75\u001B[0m                         metrics_eval_strategy\u001B[38;5;241m=\u001B[39mmetrics_eval_strategy)\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStart training [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel_type\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m] on [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdataset\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m] dataset for [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mepochs\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m] epochs with batch size [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbatch_size\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 79\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mepochs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     81\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining finished.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\0_DEV\\0_JKU\\ails_miccai\\AILS-MICCAI-UWF4DR-Challenge\\ails_miccai_uwf4dr_challenge\\models\\trainer.py:448\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, num_epochs, num_batches)\u001B[0m\n\u001B[0;32m    446\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[0;32m    447\u001B[0m     training_context\u001B[38;5;241m.\u001B[39mcurrent_epoch \u001B[38;5;241m=\u001B[39m epoch \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m--> 448\u001B[0m     model_train_results: ModelResultsAndLabels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_strategy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtraining_context\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_loader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    449\u001B[0m     model_val_results: ModelResultsAndLabels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalidation_strategy\u001B[38;5;241m.\u001B[39mvalidate(training_context, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mval_loader)\n\u001B[0;32m    451\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlr_scheduler, torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mlr_scheduler\u001B[38;5;241m.\u001B[39mReduceLROnPlateau):            \n",
      "File \u001B[1;32m~\\0_DEV\\0_JKU\\ails_miccai\\AILS-MICCAI-UWF4DR-Challenge\\ails_miccai_uwf4dr_challenge\\models\\trainer.py:339\u001B[0m, in \u001B[0;36mDefaultEpochTrainingStrategy.train\u001B[1;34m(self, training_context, train_loader)\u001B[0m\n\u001B[0;32m    336\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m    338\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m training_context\u001B[38;5;241m.\u001B[39mtimer\u001B[38;5;241m.\u001B[39mtime(Timings\u001B[38;5;241m.\u001B[39mBATCH_PROCESSING):\n\u001B[1;32m--> 339\u001B[0m     batch_results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_strategy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtraining_context\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    340\u001B[0m     results\u001B[38;5;241m.\u001B[39madd_batch_results(batch_results)\n\u001B[0;32m    342\u001B[0m loss \u001B[38;5;241m=\u001B[39m batch_results\u001B[38;5;241m.\u001B[39mmodel_results\u001B[38;5;241m.\u001B[39mloss\n",
      "File \u001B[1;32m~\\0_DEV\\0_JKU\\ails_miccai\\AILS-MICCAI-UWF4DR-Challenge\\ails_miccai_uwf4dr_challenge\\models\\trainer.py:287\u001B[0m, in \u001B[0;36mDefaultBatchTrainingStrategy.train_batch\u001B[1;34m(self, training_context, batch)\u001B[0m\n\u001B[0;32m    285\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m training_context\u001B[38;5;241m.\u001B[39mtimer\u001B[38;5;241m.\u001B[39mtime(Timings\u001B[38;5;241m.\u001B[39mFORWARD_PASS):\n\u001B[0;32m    286\u001B[0m     training_context\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m--> 287\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mtraining_context\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    289\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m training_context\u001B[38;5;241m.\u001B[39mtimer\u001B[38;5;241m.\u001B[39mtime(Timings\u001B[38;5;241m.\u001B[39mCALC_LOSS):\n\u001B[0;32m    290\u001B[0m     loss \u001B[38;5;241m=\u001B[39m training_context\u001B[38;5;241m.\u001B[39mcriterion(outputs, labels)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AILS-MICCAI-UWF4DR\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AILS-MICCAI-UWF4DR\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[14], line 22\u001B[0m, in \u001B[0;36mTask1EfficientNetB0.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m---> 22\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AILS-MICCAI-UWF4DR\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AILS-MICCAI-UWF4DR\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AILS-MICCAI-UWF4DR\\lib\\site-packages\\efficientnet_pytorch\\model.py:314\u001B[0m, in \u001B[0;36mEfficientNet.forward\u001B[1;34m(self, inputs)\u001B[0m\n\u001B[0;32m    304\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"EfficientNet's forward function.\u001B[39;00m\n\u001B[0;32m    305\u001B[0m \u001B[38;5;124;03m   Calls extract_features to extract features, applies final linear layer, and returns logits.\u001B[39;00m\n\u001B[0;32m    306\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    311\u001B[0m \u001B[38;5;124;03m    Output of this model after processing.\u001B[39;00m\n\u001B[0;32m    312\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    313\u001B[0m \u001B[38;5;66;03m# Convolution layers\u001B[39;00m\n\u001B[1;32m--> 314\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextract_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    315\u001B[0m \u001B[38;5;66;03m# Pooling and final linear layer\u001B[39;00m\n\u001B[0;32m    316\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_avg_pooling(x)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AILS-MICCAI-UWF4DR\\lib\\site-packages\\efficientnet_pytorch\\model.py:296\u001B[0m, in \u001B[0;36mEfficientNet.extract_features\u001B[1;34m(self, inputs)\u001B[0m\n\u001B[0;32m    294\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m drop_connect_rate:\n\u001B[0;32m    295\u001B[0m         drop_connect_rate \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(idx) \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_blocks)  \u001B[38;5;66;03m# scale drop connect_rate\u001B[39;00m\n\u001B[1;32m--> 296\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mblock\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdrop_connect_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdrop_connect_rate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    298\u001B[0m \u001B[38;5;66;03m# Head\u001B[39;00m\n\u001B[0;32m    299\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_swish(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bn1(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_conv_head(x)))\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AILS-MICCAI-UWF4DR\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AILS-MICCAI-UWF4DR\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AILS-MICCAI-UWF4DR\\lib\\site-packages\\efficientnet_pytorch\\model.py:107\u001B[0m, in \u001B[0;36mMBConvBlock.forward\u001B[1;34m(self, inputs, drop_connect_rate)\u001B[0m\n\u001B[0;32m    105\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expand_conv(inputs)\n\u001B[0;32m    106\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bn0(x)\n\u001B[1;32m--> 107\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_swish\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    109\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_depthwise_conv(x)\n\u001B[0;32m    110\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bn1(x)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AILS-MICCAI-UWF4DR\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AILS-MICCAI-UWF4DR\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AILS-MICCAI-UWF4DR\\lib\\site-packages\\efficientnet_pytorch\\utils.py:80\u001B[0m, in \u001B[0;36mMemoryEfficientSwish.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     79\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m---> 80\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSwishImplementation\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AILS-MICCAI-UWF4DR\\lib\\site-packages\\torch\\autograd\\function.py:598\u001B[0m, in \u001B[0;36mFunction.apply\u001B[1;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[0;32m    595\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_are_functorch_transforms_active():\n\u001B[0;32m    596\u001B[0m     \u001B[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001B[39;00m\n\u001B[0;32m    597\u001B[0m     args \u001B[38;5;241m=\u001B[39m _functorch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39munwrap_dead_wrappers(args)\n\u001B[1;32m--> 598\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m    600\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_setup_ctx_defined:\n\u001B[0;32m    601\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    602\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIn order to use an autograd.Function with functorch transforms \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    603\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    604\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstaticmethod. For more details, please see \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    605\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    606\u001B[0m     )\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\AILS-MICCAI-UWF4DR\\lib\\site-packages\\efficientnet_pytorch\\utils.py:67\u001B[0m, in \u001B[0;36mSwishImplementation.forward\u001B[1;34m(ctx, i)\u001B[0m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(ctx, i):\n\u001B[1;32m---> 67\u001B[0m     result \u001B[38;5;241m=\u001B[39m i \u001B[38;5;241m*\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msigmoid\u001B[49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     68\u001B[0m     ctx\u001B[38;5;241m.\u001B[39msave_for_backward(i)\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
