{
 "cells": [
  {
   "metadata": {
    "id": "ehS5McPJTqO7"
   },
   "cell_type": "markdown",
   "source": [
    "### Train for MICCAI challenge on colab using data on gDrive"
   ]
  },
  {
   "metadata": {
    "id": "v92Q3jB2TqO8",
    "outputId": "03cc383a-058b-401c-c83f-4eb38131ee0d",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\r0% [Working]\r            \rHit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
      "\r0% [Connecting to archive.ubuntu.com] [Connected to cloud.r-project.org (18.165.98.93)] [Connecting \r                                                                                                    \rHit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "\r0% [Connected to cloud.r-project.org (18.165.98.93)] [Connecting to ppa.launchpadcontent.net (185.12\r                                                                                                    \rHit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
      "\r0% [Waiting for headers] [Connected to cloud.r-project.org (18.165.98.93)] [Connecting to ppa.launch\r                                                                                                    \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Hit:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
      "Hit:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
      "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
      "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
      "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
      "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "git is already the newest version (1:2.34.1-1ubuntu1.11).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 52 not upgraded.\n",
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
      "Requirement already satisfied: loguru in /usr/local/lib/python3.10/dist-packages (0.7.2)\n"
     ]
    }
   ],
   "execution_count": 8,
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from ails_miccai_uwf4dr_challenge.augmentations import rotate_affine_flip_choice, resize_only\n",
    "from ails_miccai_uwf4dr_challenge.models.metrics import sensitivity_score, specificity_score\n",
    "from ails_miccai_uwf4dr_challenge.models.trainer import Metric, DefaultMetricsEvaluationStrategy, Trainer\n",
    "# setup\n",
    "!apt-get update\n",
    "!apt-get install git\n",
    "!pip install python-dotenv\n",
    "!pip install loguru"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# clone repo in order to have modules available\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "# Define the parameters\n",
    "username = \"bscheuringer\"\n",
    "access_token = \"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\"\n",
    "repo_name = \"AILS-MICCAI-UWF4DR-Challenge\"\n",
    "repo_clone_url = f\"https://{username}:{access_token}@github.com/moritsih/{repo_name}.git\"\n",
    "repo_path = f'/content/{repo_name}'\n",
    "\n",
    "# Check if the repository already exists\n",
    "if not os.path.isdir(repo_path):\n",
    "    !git clone {repo_clone_url}\n",
    "else:\n",
    "    print(\"Repository already exists.\")\n",
    "\n",
    "# navigate to repo directory in order to have working imports\n",
    "%cd {repo_path}\n",
    "\n",
    "!git checkout bsc_colab  # TODO remove when branch is not needed anymore\n",
    "\n",
    "# add repo path to sys path\n",
    "if repo_path not in sys.path:\n",
    "    sys.path.append(repo_path)\n",
    "\n",
    "# Print sys.path to verify\n",
    "print(\"Python Path:\", sys.path)"
   ],
   "metadata": {
    "id": "1Np1qcT1gHgs",
    "outputId": "21015545-44ce-4136-a288-f170049d5519",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Repository already exists.\n",
      "/content/AILS-MICCAI-UWF4DR-Challenge\n",
      "Already on 'bsc_colab'\n",
      "Your branch is up to date with 'origin/bsc_colab'.\n",
      "Python Path: ['/content', '/env/python', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.10/dist-packages/IPython/extensions', '/root/.ipython', '/content/AILS-MICCAI-UWF4DR-Challenge']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# load data and unzip data\n",
    "!python ./tools/download_data_and_chkpts.py"
   ],
   "metadata": {
    "id": "g6Z1v3t65Orz",
    "outputId": "9894eab7-18ae-4f14-91a4-f0f3ca164c0c",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading from 'https://drive.google.com/uc?id=1jm48RSCctyxtEkppS45Znh0wtdf9patA' to 'data/downloads/DeepDRiD.zip.enc'\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1jm48RSCctyxtEkppS45Znh0wtdf9patA\n",
      "From (redirected): https://drive.google.com/uc?id=1jm48RSCctyxtEkppS45Znh0wtdf9patA&confirm=t&uuid=cbe223af-61b2-4423-8221-115b696268cf\n",
      "To: /content/AILS-MICCAI-UWF4DR-Challenge/data/downloads/DeepDRiD.zip.enc\n",
      "100% 303M/303M [00:09<00:00, 32.7MB/s]\n",
      "Downloaded to 'data/downloads/DeepDRiD.zip.enc'\n",
      "Decrypted from 'data/downloads/DeepDRiD.zip.enc' to 'data/downloads/DeepDRiD.zip'\n",
      "Extracting 'data/downloads/DeepDRiD.zip' to 'data/external'\n",
      "Downloading from 'https://drive.google.com/uc?id=1K8xwscXQQo0KXEzFaC2wybgD-UYNXvfc' to 'data/downloads/UWF4DRChallengeData.zip.enc'\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1K8xwscXQQo0KXEzFaC2wybgD-UYNXvfc\n",
      "From (redirected): https://drive.google.com/uc?id=1K8xwscXQQo0KXEzFaC2wybgD-UYNXvfc&confirm=t&uuid=b79e7f9e-d957-4794-9916-c4a7805b3e1b\n",
      "To: /content/AILS-MICCAI-UWF4DR-Challenge/data/downloads/UWF4DRChallengeData.zip.enc\n",
      "100% 150M/150M [00:02<00:00, 71.4MB/s]\n",
      "Downloaded to 'data/downloads/UWF4DRChallengeData.zip.enc'\n",
      "Decrypted from 'data/downloads/UWF4DRChallengeData.zip.enc' to 'data/downloads/UWF4DRChallengeData.zip'\n",
      "Extracting 'data/downloads/UWF4DRChallengeData.zip' to 'data/raw'\n",
      "Downloading from 'https://drive.google.com/uc?id=1t7Dt8ViDAZ4fLYsFBWmU12Smv10hJyLo' to 'data/downloads/automorph_best_loss_checkpoint.pth'\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1t7Dt8ViDAZ4fLYsFBWmU12Smv10hJyLo\n",
      "From (redirected): https://drive.google.com/uc?id=1t7Dt8ViDAZ4fLYsFBWmU12Smv10hJyLo&confirm=t&uuid=6e792217-d659-43f1-b4e7-88a2625c2020\n",
      "To: /content/AILS-MICCAI-UWF4DR-Challenge/data/downloads/automorph_best_loss_checkpoint.pth\n",
      "100% 72.9M/72.9M [00:00<00:00, 88.8MB/s]\n",
      "Downloaded to 'data/downloads/automorph_best_loss_checkpoint.pth'\n",
      "File 'data/downloads/automorph_best_loss_checkpoint.pth' is not encrypted. Skipping decryption.\n",
      "Skipping extraction for 'data/downloads/automorph_best_loss_checkpoint.pth'\n",
      "Moving 'data/downloads/automorph_best_loss_checkpoint.pth' to 'models/AutoMorph'\n",
      "Moved to 'models/AutoMorph/automorph_best_loss_checkpoint.pth'\n",
      "Downloading from 'https://drive.google.com/uc?id=1X5gekt4_BbIZLoj2fVMHwjWRRGWotzm1' to 'data/downloads/model_weights.pth'\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1X5gekt4_BbIZLoj2fVMHwjWRRGWotzm1\n",
      "To: /content/AILS-MICCAI-UWF4DR-Challenge/data/downloads/model_weights.pth\n",
      "100% 85.3M/85.3M [00:02<00:00, 29.5MB/s]\n",
      "Downloaded to 'data/downloads/model_weights.pth'\n",
      "File 'data/downloads/model_weights.pth' is not encrypted. Skipping decryption.\n",
      "Skipping extraction for 'data/downloads/model_weights.pth'\n",
      "File 'models/submission_eval_model_weights/model_weights.pth' already exists. Skipping move.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# test repo import\n",
    "!ls {repo_path}\n",
    "\n",
    "# try importing a custom class\n",
    "try:\n",
    "    from ails_miccai_uwf4dr_challenge.dataset import DatasetBuilder, CustomDataset\n",
    "\n",
    "    print(\"Import successful!\")\n",
    "except ImportError as e:\n",
    "    print(\"Import failed:\", e)"
   ],
   "metadata": {
    "id": "j-0jnXXgnArd",
    "outputId": "2e3e1d5b-78c2-4c8f-c676-d847a86f150a",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "aes256.key\t\t      docs\tnotebooks\treferences\t  tests\n",
      "ails_miccai_uwf4dr_challenge  Makefile\tpyproject.toml\treports\t\t  tools\n",
      "data\t\t\t      models\tREADME.md\trequirements.txt  wandb\n",
      "Import successful!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Gy3zF2c9TqO9"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from ails_miccai_uwf4dr_challenge.dataset import DatasetBuilder\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "metadata": {
    "id": "muTlnyjwTqO9",
    "outputId": "e78aeecb-1295-4cc8-cc09-58e65448dd89",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "execution_count": 14,
   "source": [
    "# connect to gDrive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "ails_data_base_path = Path(\"/content/drive/shared-with-me/data\")\n",
    "my_data_base_path = Path(\"/content/drive/My Drive/JKU/AILS_CHALLENGE_2024\")"
   ]
  },
  {
   "metadata": {
    "id": "t0wClbAUTqO-",
    "outputId": "5b2333f5-cedb-43a8-9378-40d155af9bae",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "execution_count": 15,
   "source": [
    "# select device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \" + str(device))"
   ]
  },
  {
   "metadata": {
    "id": "v3cpVuVrTqO-",
    "outputId": "9bdfd763-e427-4535-a711-3c1b6a52ad1d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Invalid dataset name: all",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-16-86189f40cbdf>\u001B[0m in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;31m#external_data_dir = ails_data_base_path / \"external\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m#dataset_builder = DatasetBuilder(dataset='all', task='task1', original_data_dir=original_data_dir, external_data_dir=external_data_dir).get_train_val()\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mdataset_builder\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mDatasetBuilder\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'all'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'task1'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/content/AILS-MICCAI-UWF4DR-Challenge/ails_miccai_uwf4dr_challenge/dataset.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, dataset, task, split_ratio, get_mini_dataset, frac, original_data_dir, external_data_dir)\u001B[0m\n\u001B[1;32m    237\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    238\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 239\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"Invalid dataset name: {dataset}\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    240\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    241\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: Invalid dataset name: all"
     ]
    }
   ],
   "execution_count": 16,
   "source": [
    "# setup dataset\n",
    "#original_data_dir = ails_data_base_path / \"raw\"\n",
    "#external_data_dir = ails_data_base_path / \"external\"\n",
    "#dataset_builder = DatasetBuilder(dataset='all', task='task1', original_data_dir=original_data_dir, external_data_dir=external_data_dir).get_train_val()\n",
    "dataset_builder = DatasetBuilder(dataset='all', task='task1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YkuNXL9zTqO-",
    "outputId": "52419450-bde5-4c0f-ab37-859ca7a05401"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length:  508\n",
      "Dataset length:  128\n"
     ]
    }
   ],
   "source": [
    "# train EfficientNet for image quality assessment (Task 1)\n",
    "from ails_miccai_uwf4dr_challenge.models.architectures.task1_efficientnet_plain import Task1EfficientNetB4\n",
    "model = Task1EfficientNetB4()\n",
    "model.to(device)\n",
    "print(\"Training model: \", model.__class__.__name__)   \n",
    "\n",
    "metrics = [\n",
    "        Metric('auroc', roc_auc_score),\n",
    "        Metric('auprc', average_precision_score),\n",
    "        Metric('accuracy', lambda y_true, y_pred: (y_pred.round() == y_true).mean()),\n",
    "        Metric('sensitivity', sensitivity_score),\n",
    "        Metric('specificity', specificity_score)\n",
    "    ]\n",
    "\n",
    "metrics_eval_strategy = DefaultMetricsEvaluationStrategy(metrics)\n",
    "\n",
    "config = {\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"dataset\": \"UWF4DR-Original\",\n",
    "    \"epochs\": 15,\n",
    "    \"batch_size\": 4,\n",
    "    \"model_type\": model.__class__.__name__ \n",
    "}\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config[\"learning_rate\"])\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "train_data, val_data = dataset_builder.get_train_val()\n",
    "train_dataset = CustomDataset(train_data, transform=rotate_affine_flip_choice)\n",
    "val_dataset = CustomDataset(val_data, transform=resize_only)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "trainer = Trainer(model, train_loader, val_loader, criterion, optimizer, lr_scheduler, device, \n",
    "                        metrics_eval_strategy=metrics_eval_strategy)\n",
    "\n",
    "print(f\"Start training [{config['model_type']}] on [{config['dataset']}] dataset for [{config['epochs']}] epochs with batch size [{config['batch_size']}]\")\n",
    "    \n",
    "trainer.train(num_epochs=config[\"epochs\"])\n",
    "\n",
    "print(\"Training finished.\")\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AILS-MICCAI-UWF4DR-Challenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
