{
 "cells": [
  {
   "metadata": {
    "id": "ehS5McPJTqO7"
   },
   "cell_type": "markdown",
   "source": [
    "### Train for MICCAI challenge on colab using data on gDrive"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Gy3zF2c9TqO9",
    "ExecuteTime": {
     "end_time": "2024-07-24T06:26:39.876469Z",
     "start_time": "2024-07-24T06:26:31.157351Z"
    }
   },
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "from ails_miccai_uwf4dr_challenge.models.metrics import sensitivity_score, specificity_score\n",
    "from ails_miccai_uwf4dr_challenge.models.trainer import Metric, DefaultMetricsEvaluationStrategy, Trainer, TrainingContext, MetricCalculatedHook, PersistBestModelOnEpochEndHook"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "id": "t0wClbAUTqO-",
    "outputId": "c5a3ffde-940f-424a-85b7-69b3455735bf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-07-24T06:26:59.326950Z",
     "start_time": "2024-07-24T06:26:59.307482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# select device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \" + str(device))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "id": "v3cpVuVrTqO-",
    "ExecuteTime": {
     "end_time": "2024-07-24T06:27:01.782997Z",
     "start_time": "2024-07-24T06:27:00.344795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ails_miccai_uwf4dr_challenge.dataset_strategy import CombinedDatasetStrategy, Task1Strategy, TrainValSplitStrategy, \\\n",
    "    RandomOverSamplingStrategy, DatasetBuilder, CustomDataset\n",
    "\n",
    "# setup dataset\n",
    "\n",
    "dataset_strategy = CombinedDatasetStrategy()\n",
    "task_strategy = Task1Strategy()\n",
    "\n",
    "split_strategy = TrainValSplitStrategy(split_ratio=0.8)\n",
    "resampling_strategy = RandomOverSamplingStrategy()\n",
    "\n",
    "# Build dataset\n",
    "dataset_builder = DatasetBuilder(dataset_strategy, task_strategy, split_strategy, resampling_strategy)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage import restoration\n",
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "class GreenChannelEnhancement:\n",
    "    def __call__(self, img):\n",
    "        # Convert to numpy array if it's a tensor\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = img.numpy().transpose((1, 2, 0))\n",
    "\n",
    "        # Ensure the image is in the correct format\n",
    "        img = img.astype(np.float32)\n",
    "\n",
    "        # Separate the channels\n",
    "        r, g, b = cv2.split(img)\n",
    "\n",
    "        # Apply Wiener filter to the green channel\n",
    "        psf = np.ones((5, 5)) / 25\n",
    "        g_filtered = restoration.wiener(g, psf, balance=0.1)\n",
    "\n",
    "        # Apply CLAHE to the filtered green channel\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        g_enhanced = clahe.apply((g_filtered * 255).astype(np.uint8))\n",
    "        g_enhanced = g_enhanced / 255.0  # Normalize back to range [0, 1]\n",
    "\n",
    "        # Ensure all channels are the same type\n",
    "        r = r.astype(np.float32)\n",
    "        g_enhanced = g_enhanced.astype(np.float32)\n",
    "        b = b.astype(np.float32)\n",
    "\n",
    "        # Merge the enhanced green channel back with the original red and blue channels\n",
    "        enhanced_img = cv2.merge((r, g_enhanced, b))\n",
    "\n",
    "        # Convert back to tensor\n",
    "        enhanced_img = torch.from_numpy(enhanced_img.transpose((2, 0, 1)))\n",
    "        return enhanced_img"
   ],
   "metadata": {
    "id": "5zRh669DvMSI",
    "ExecuteTime": {
     "end_time": "2024-07-24T06:27:05.740297Z",
     "start_time": "2024-07-24T06:27:03.016851Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T06:32:15.117026Z",
     "start_time": "2024-07-24T06:32:15.102561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class CropFundus:\n",
    "    def __init__(self, target_size=(448, 448)):\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = img.permute(1, 2, 0).numpy()  # Convert tensor to numpy array in HWC format\n",
    "\n",
    "        # Ensure the image is in the correct format\n",
    "        img = img.astype(np.uint8)\n",
    "\n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Apply threshold to get binary image\n",
    "        _, binary = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        # Find contours\n",
    "        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        print(f'---> found [{len(contours)}] contours')\n",
    "\n",
    "        if contours:\n",
    "            # Find the largest contour\n",
    "            largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "            # Get bounding box for the largest contour\n",
    "            x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "\n",
    "            # Crop the image using the bounding box\n",
    "            cropped_img = img[y:y+h, x:x+w]\n",
    "        else:\n",
    "            cropped_img = img  # If no contours found, use the original image\n",
    "\n",
    "        # Resize the cropped image to the target size\n",
    "        resized_img = cv2.resize(cropped_img, self.target_size)\n",
    "\n",
    "        # Debugging prints\n",
    "        print(\"Original image shape:\", img.shape)\n",
    "        print(\"Cropped image shape:\", cropped_img.shape)\n",
    "        print(\"Resized image shape:\", resized_img.shape)\n",
    "        print(\"Max pixel value before normalization:\", np.max(resized_img))\n",
    "        print(\"Min pixel value before normalization:\", np.min(resized_img))\n",
    "\n",
    "        # Ensure the image is in the correct format and normalize pixel values\n",
    "        #resized_img = resized_img.astype(np.float32) / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "        # Convert back to tensor\n",
    "        resized_img = torch.from_numpy(resized_img).permute(2, 0, 1).float()  # Convert back to tensor\n",
    "\n",
    "        # Debugging prints\n",
    "        print(\"Max pixel value after normalization:\", torch.max(resized_img).item())\n",
    "        print(\"Min pixel value after normalization:\", torch.min(resized_img).item())\n",
    "\n",
    "        return resized_img"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "id": "ZJGshtZshesJ",
    "ExecuteTime": {
     "end_time": "2024-07-24T06:32:15.684472Z",
     "start_time": "2024-07-24T06:32:15.659779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision.transforms import v2\n",
    "import torch\n",
    "\n",
    "just_to_image = v2.Compose([\n",
    "    v2.ToPILImage(),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True)\n",
    "])\n",
    "\n",
    "# use this augmentation pipeline in the case of:\n",
    "# 1. training\n",
    "# 2. both datasets are included (therefore: resizing or cropping)\n",
    "augment_for_task_1_training = v2.Compose([\n",
    "    v2.ToPILImage(),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    #GreenChannelEnhancement(),\n",
    "    #v2.ColorJitter(brightness=0.5, contrast=0.4, saturation=0.3, hue=0.3),\n",
    "    #v2.RandomHorizontalFlip(),\n",
    "\n",
    "    #v2.RandomVerticalFlip(),\n",
    "    #v2.RandomRotation(degrees=15, expand=True),\n",
    "    #v2.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    #v2.Resize(size=(224, 224), antialias=True),\n",
    "    #v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "augment_for_task_1_validation = v2.Compose([\n",
    "    v2.ToPILImage(),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    GreenChannelEnhancement(),\n",
    "    #v2.ColorJitter(brightness=0.5, contrast=0.4, saturation=0.3, hue=0.3),\n",
    "    #v2.RandomHorizontalFlip(),\n",
    "    #v2.RandomVerticalFlip(),\n",
    "    #v2.RandomRotation(degrees=15, expand=True),\n",
    "    #v2.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    v2.Resize(size=(224, 224), antialias=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_data, val_data = dataset_builder.build()\n",
    "train_dataset = CustomDataset(train_data, just_to_image)\n",
    "img_data_loader = DataLoader(train_dataset)\n",
    "\n",
    "# Function to display images\n",
    "def show_images(images, titles=None):\n",
    "    n = len(images)\n",
    "    fig, axs = plt.subplots(1, n, figsize=(20, 5))\n",
    "    if n == 1:\n",
    "        axs = [axs]\n",
    "    for i, img in enumerate(images):\n",
    "        img = img.permute(1, 2, 0).numpy()  # Convert to HxWxC for visualization\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        axs[i].imshow(img)\n",
    "        axs[i].axis('off')\n",
    "        if titles:\n",
    "            axs[i].set_title(titles[i])\n",
    "    plt.show()\n",
    "\n",
    "# Display first 5 images from train_loader before and after GreenChannelEnhancement\n",
    "images_before = []\n",
    "images_after = []\n",
    "labels = []\n",
    "\n",
    "# Get 5 images from the loader\n",
    "for i, (img, label) in enumerate(img_data_loader):\n",
    "    if i == 3:\n",
    "        break\n",
    "    images_before.append(img[0].clone())  # Store a clone of the original image\n",
    "    #transformed_img = GreenChannelEnhancement()(img[0])  # Apply the enhancement\n",
    "    transformed_img = CropFundus()(img[0].clone())\n",
    "    images_after.append(transformed_img)  # Store the transformed image\n",
    "    labels.append(label[0].item())\n",
    "\n",
    "# Show original images\n",
    "print(\"Original Images:\")\n",
    "show_images(images_before, titles=labels)\n",
    "\n",
    "# Show enhanced images\n",
    "print(\"Enhanced Images:\")\n",
    "show_images(images_after, titles=labels)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "WcAHLBXTw0rI",
    "outputId": "2690e1bc-f212-4211-f952-ca64693ba67f",
    "ExecuteTime": {
     "end_time": "2024-07-24T06:32:18.018763Z",
     "start_time": "2024-07-24T06:32:16.187395Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original class distribution:\n",
      "Class 0: 209\n",
      "Class 1: 427\n",
      "quality\n",
      "1    0.671384\n",
      "0    0.328616\n",
      "Name: proportion, dtype: Float64\n",
      "\n",
      "Resampled class distribution:\n",
      "quality\n",
      "1    0.5\n",
      "0    0.5\n",
      "Name: proportion, dtype: Float64\n",
      "---> found [0] contours\n",
      "Original image shape: (800, 1016, 3)\n",
      "Cropped image shape: (800, 1016, 3)\n",
      "Resized image shape: (448, 448, 3)\n",
      "Max pixel value before normalization: 1\n",
      "Min pixel value before normalization: 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "max(): argument 'input' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 33\u001B[0m\n\u001B[0;32m     31\u001B[0m images_before\u001B[38;5;241m.\u001B[39mappend(img[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mclone())  \u001B[38;5;66;03m# Store a clone of the original image\u001B[39;00m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;66;03m#transformed_img = GreenChannelEnhancement()(img[0])  # Apply the enhancement\u001B[39;00m\n\u001B[1;32m---> 33\u001B[0m transformed_img \u001B[38;5;241m=\u001B[39m \u001B[43mCropFundus\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     34\u001B[0m images_after\u001B[38;5;241m.\u001B[39mappend(transformed_img)  \u001B[38;5;66;03m# Store the transformed image\u001B[39;00m\n\u001B[0;32m     35\u001B[0m labels\u001B[38;5;241m.\u001B[39mappend(label[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mitem())\n",
      "Cell \u001B[1;32mIn[14], line 55\u001B[0m, in \u001B[0;36mCropFundus.__call__\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMin pixel value before normalization:\u001B[39m\u001B[38;5;124m\"\u001B[39m, np\u001B[38;5;241m.\u001B[39mmin(resized_img))\n\u001B[0;32m     48\u001B[0m \u001B[38;5;66;03m# Ensure the image is in the correct format and normalize pixel values\u001B[39;00m\n\u001B[0;32m     49\u001B[0m \u001B[38;5;66;03m#resized_img = resized_img.astype(np.float32) / 255.0  # Normalize to [0, 1]\u001B[39;00m\n\u001B[0;32m     50\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     53\u001B[0m \n\u001B[0;32m     54\u001B[0m \u001B[38;5;66;03m# Debugging prints\u001B[39;00m\n\u001B[1;32m---> 55\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMax pixel value after normalization:\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresized_img\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mitem())\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMin pixel value after normalization:\u001B[39m\u001B[38;5;124m\"\u001B[39m, torch\u001B[38;5;241m.\u001B[39mmin(resized_img)\u001B[38;5;241m.\u001B[39mitem())\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resized_img\n",
      "\u001B[1;31mTypeError\u001B[0m: max(): argument 'input' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "colab": {
   "provenance": [],
   "gpuType": "L4",
   "machine_shape": "hm"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
