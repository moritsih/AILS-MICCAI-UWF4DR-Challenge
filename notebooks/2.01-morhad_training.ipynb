{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../AILS-MICCAI-UWF4DR-Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting typer\n",
      "  Downloading typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer) (8.1.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from typer) (4.10.0)\n",
      "Collecting shellingham>=1.3.0 (from typer)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer) (13.7.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer) (0.1.2)\n",
      "Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m198.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Installing collected packages: shellingham, typer\n",
      "\u001b[33m  WARNING: The script typer is installed in '/home/optima/mhaderer/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed shellingham-1.5.4 typer-0.12.3\n"
     ]
    }
   ],
   "source": [
    "# setup\n",
    "#!apt-get update\n",
    "#!pip install python-dotenv\n",
    "#!pip install loguru\n",
    "#!pip install gdown\n",
    "#!pip install typer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from 'https://drive.google.com/uc?id=1jm48RSCctyxtEkppS45Znh0wtdf9patA' to 'data/downloads/DeepDRiD.zip.enc'\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1jm48RSCctyxtEkppS45Znh0wtdf9patA\n",
      "From (redirected): https://drive.google.com/uc?id=1jm48RSCctyxtEkppS45Znh0wtdf9patA&confirm=t&uuid=071ccac2-f501-40a2-a483-c2e246db8916\n",
      "To: /home/optima/mhaderer/AILS-MICCAI-UWF4DR-Challenge/data/downloads/DeepDRiD.zip.enc\n",
      "100%|████████████████████████████████████████| 303M/303M [00:03<00:00, 93.0MB/s]\n",
      "Downloaded to 'data/downloads/DeepDRiD.zip.enc'\n",
      "Decrypted from 'data/downloads/DeepDRiD.zip.enc' to 'data/downloads/DeepDRiD.zip'\n",
      "Extracting 'data/downloads/DeepDRiD.zip' to 'data/external'\n",
      "Downloading from 'https://drive.google.com/uc?id=1K8xwscXQQo0KXEzFaC2wybgD-UYNXvfc' to 'data/downloads/UWF4DRChallengeData.zip.enc'\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1K8xwscXQQo0KXEzFaC2wybgD-UYNXvfc\n",
      "From (redirected): https://drive.google.com/uc?id=1K8xwscXQQo0KXEzFaC2wybgD-UYNXvfc&confirm=t&uuid=1c1b6c43-5e02-4329-8b20-2a297098d5ec\n",
      "To: /home/optima/mhaderer/AILS-MICCAI-UWF4DR-Challenge/data/downloads/UWF4DRChallengeData.zip.enc\n",
      "100%|████████████████████████████████████████| 150M/150M [00:02<00:00, 70.9MB/s]\n",
      "Downloaded to 'data/downloads/UWF4DRChallengeData.zip.enc'\n",
      "Decrypted from 'data/downloads/UWF4DRChallengeData.zip.enc' to 'data/downloads/UWF4DRChallengeData.zip'\n",
      "Extracting 'data/downloads/UWF4DRChallengeData.zip' to 'data/raw'\n",
      "Downloading from 'https://drive.google.com/uc?id=1t7Dt8ViDAZ4fLYsFBWmU12Smv10hJyLo' to 'data/downloads/automorph_best_loss_checkpoint.pth'\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1t7Dt8ViDAZ4fLYsFBWmU12Smv10hJyLo\n",
      "From (redirected): https://drive.google.com/uc?id=1t7Dt8ViDAZ4fLYsFBWmU12Smv10hJyLo&confirm=t&uuid=e5f21903-b266-4c2a-ab05-74332c4396ed\n",
      "To: /home/optima/mhaderer/AILS-MICCAI-UWF4DR-Challenge/data/downloads/automorph_best_loss_checkpoint.pth\n",
      "100%|██████████████████████████████████████| 72.9M/72.9M [00:01<00:00, 49.9MB/s]\n",
      "Downloaded to 'data/downloads/automorph_best_loss_checkpoint.pth'\n",
      "File 'data/downloads/automorph_best_loss_checkpoint.pth' is not encrypted. Skipping decryption.\n",
      "Skipping extraction for 'data/downloads/automorph_best_loss_checkpoint.pth'\n",
      "Moving 'data/downloads/automorph_best_loss_checkpoint.pth' to 'models/AutoMorph'\n",
      "Moved to 'models/AutoMorph/automorph_best_loss_checkpoint.pth'\n",
      "Downloading from 'https://drive.google.com/uc?id=1X5gekt4_BbIZLoj2fVMHwjWRRGWotzm1' to 'data/downloads/model_weights.pth'\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1X5gekt4_BbIZLoj2fVMHwjWRRGWotzm1\n",
      "To: /home/optima/mhaderer/AILS-MICCAI-UWF4DR-Challenge/data/downloads/model_weights.pth\n",
      "100%|██████████████████████████████████████| 85.3M/85.3M [00:05<00:00, 15.7MB/s]\n",
      "Downloaded to 'data/downloads/model_weights.pth'\n",
      "File 'data/downloads/model_weights.pth' is not encrypted. Skipping decryption.\n",
      "Skipping extraction for 'data/downloads/model_weights.pth'\n",
      "File 'models/submission_eval_model_weights/model_weights.pth' already exists. Skipping move.\n"
     ]
    }
   ],
   "source": [
    "# load data and unzip data\n",
    "\n",
    "!python tools/download_data_and_chkpts.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from ails_miccai_uwf4dr_challenge.dataset import DatasetBuilder, DatasetOriginationType, ChallengeTaskType, CustomDataset\n",
    "\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "from ails_miccai_uwf4dr_challenge.preprocess_augmentations import ResidualGaussBlur, MultiplyMask\n",
    "from ails_miccai_uwf4dr_challenge.augmentations import rotate_affine_flip_choice, resize_only\n",
    "from ails_miccai_uwf4dr_challenge.models.metrics import sensitivity_score, specificity_score\n",
    "from ails_miccai_uwf4dr_challenge.models.trainer import Metric, DefaultMetricsEvaluationStrategy, Trainer, TrainingContext, MetricCalculatedHook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403388920/work/c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# select device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \" + str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "preprocessing = A.Compose([\n",
    "        A.Resize(800, 1016, p=1),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], p=1),\n",
    "        #ResidualGaussBlur(p=1),\n",
    "        MultiplyMask(p=1),\n",
    "        A.Resize(800, 1016, p=1),\n",
    "        #A.Equalize(p=0.1),\n",
    "        #A.CLAHE(clip_limit=5., p=0.3)\n",
    "    ])\n",
    "\n",
    "augment_train = A.Compose([\n",
    "        #A.VerticalFlip(p=0.5),\n",
    "        #A.HorizontalFlip(p=0.5),\n",
    "        #A.Affine(rotate=15, rotate_method='ellipse', p=0.5),\n",
    "        ToTensorV2(p=1)\n",
    "    ])\n",
    "\n",
    "augment_val = A.Compose([\n",
    "        ToTensorV2(p=1)\n",
    "    ])\n",
    "\n",
    "\n",
    "transforms_train = A.Compose([\n",
    "    preprocessing,\n",
    "    augment_train\n",
    "])\n",
    "\n",
    "transforms_val = A.Compose([\n",
    "    preprocessing,\n",
    "    augment_val\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ails_miccai_uwf4dr_challenge.models.architectures.task1_automorph_plain import AutoMorphModel\n",
    "\n",
    "model = AutoMorphModel(enc_frozen=True)\n",
    "\n",
    "model.to(device)\n",
    "print(\"Training model: \", model.__class__.__name__)\n",
    "\n",
    "metrics = [\n",
    "        Metric('auroc', roc_auc_score),\n",
    "        Metric('auprc', average_precision_score),\n",
    "        Metric('accuracy', lambda y_true, y_pred: (y_pred.round() == y_true).mean()),\n",
    "        Metric('sensitivity', sensitivity_score),\n",
    "        Metric('specificity', specificity_score)\n",
    "    ]\n",
    "\n",
    "class WandbLoggingHook(MetricCalculatedHook):\n",
    "        def on_metric_calculated(self, training_context: TrainingContext, metric: Metric, result, last_metric_for_epoch: bool):\n",
    "            import wandb\n",
    "            wandb.log(data={metric.name: result}, commit=last_metric_for_epoch)\n",
    "\n",
    "metrics_eval_strategy = DefaultMetricsEvaluationStrategy(metrics)\n",
    "if(use_wandb):\n",
    "    metrics_eval_strategy.register_metric_calculated_hook(WandbLoggingHook())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length:  319\n",
      "Dataset length:  80\n",
      "Start training [AutoMorphModel] on [Original] dataset for [10] epochs with batch size [16]\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "config = {\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"dataset\": \"Original\",\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 16,\n",
    "    \"model_type\": model.__class__.__name__\n",
    "}\n",
    "\n",
    "def combined_losses(pred, target):\n",
    "    bce = F.binary_cross_entropy_with_logits(pred, target) * 0.5\n",
    "    smooth_l1 = F.smooth.l1_loss(pred, target) * 0.5\n",
    "    return bce + smooth_l1\n",
    "\n",
    "criterion = combined_losses\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config[\"learning_rate\"])\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "\n",
    "dataset_builder = DatasetBuilder(DatasetOriginationType.ORIGINAL, ChallengeTaskType.TASK2)\n",
    "\n",
    "train_data, val_data = dataset_builder.get_train_val()\n",
    "train_dataset = CustomDataset(train_data, transform=transforms_train)\n",
    "val_dataset = CustomDataset(val_data, transform=transforms_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "\n",
    "trainer = Trainer(model, train_loader, val_loader, criterion, optimizer, lr_scheduler, device,\n",
    "                        metrics_eval_strategy=metrics_eval_strategy)\n",
    "\n",
    "if use_wandb:\n",
    "    pass\n",
    "    #wandb.init(entity='miccai-challenge-2024' ,project='task2', config=config)\n",
    "\n",
    "print(f\"Start training [{config['model_type']}] on [{config['dataset']}] dataset for [{config['epochs']}] epochs with batch size [{config['batch_size']}]\")\n",
    "\n",
    "#trainer.train(num_epochs=config[\"epochs\"])\n",
    "\n",
    "print(\"Training finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
