{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "!apt-get update\n",
    "!apt-get install git\n",
    "!pip install python-dotenv\n",
    "!pip install loguru\n",
    "!pip install efficientnet_pytorch\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone repo in order to have modules available\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "# Define the parameters\n",
    "username = \"bscheuringer\"\n",
    "access_token = \"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\"\n",
    "repo_name = \"AILS-MICCAI-UWF4DR-Challenge\"\n",
    "repo_clone_url = f\"https://{username}:{access_token}@github.com/moritsih/{repo_name}.git\"\n",
    "repo_path = f'/content/{repo_name}'\n",
    "\n",
    "# Check if the repository already exists\n",
    "if not os.path.isdir(repo_path):\n",
    "    !git clone {repo_clone_url}\n",
    "else:\n",
    "    print(\"Repository already exists.\")\n",
    "\n",
    "# navigate to repo directory in order to have working imports\n",
    "%cd {repo_path}\n",
    "\n",
    "!git checkout bsc_colab  # TODO remove when branch is not needed anymore\n",
    "\n",
    "# add repo path to sys path\n",
    "if repo_path not in sys.path:\n",
    "    sys.path.append(repo_path)\n",
    "\n",
    "# Print sys.path to verify\n",
    "print(\"Python Path:\", sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and unzip data\n",
    "!python ./tools/download_data_and_chkpts.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test repo import\n",
    "!ls {repo_path}\n",
    "\n",
    "# try importing a custom class\n",
    "try:\n",
    "    from ails_miccai_uwf4dr_challenge.dataset import DatasetBuilder, CustomDataset\n",
    "\n",
    "    print(\"Import successful!\")\n",
    "except ImportError as e:\n",
    "    print(\"Import failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from ails_miccai_uwf4dr_challenge.dataset import DatasetBuilder, DatasetOriginationType, ChallengeTaskType, CustomDataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from ails_miccai_uwf4dr_challenge.augmentations import rotate_affine_flip_choice, resize_only\n",
    "from ails_miccai_uwf4dr_challenge.models.metrics import sensitivity_score, specificity_score\n",
    "from ails_miccai_uwf4dr_challenge.models.trainer import Metric, DefaultMetricsEvaluationStrategy, Trainer, TrainingContext, MetricCalculatedHook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \" + str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# login to wandb\n",
    "use_wandb = True\n",
    "if use_wandb:\n",
    "    import wandb\n",
    "    #wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup dataset\n",
    "#original_data_dir = ails_data_base_path / \"raw\"\n",
    "#external_data_dir = ails_data_base_path / \"external\"\n",
    "#dataset_builder = DatasetBuilder(dataset='all', task='task1', original_data_dir=original_data_dir, external_data_dir=external_data_dir).get_train_val()\n",
    "dataset_builder = DatasetBuilder(DatasetOriginationType.ALL, ChallengeTaskType.TASK1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "import torch\n",
    "\n",
    "# use this augmentation pipeline in the case of:\n",
    "# 1. training\n",
    "# 2. both datasets are included (therefore: resizing or cropping)\n",
    "augment_for_task_1_training = v2.Compose([\n",
    "    v2.ToPILImage(),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    #v2.ColorJitter(brightness=0.5, contrast=0.4, saturation=0.3, hue=0.3),\n",
    "    #v2.RandomHorizontalFlip(),\n",
    "    v2.RandomVerticalFlip(),\n",
    "    v2.RandomRotation(degrees=15, expand=True),\n",
    "    #v2.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    v2.Resize(size=(800, 1016), antialias=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "augment_for_task_1_validation = v2.Compose([\n",
    "    v2.ToPILImage(),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    #v2.ColorJitter(brightness=0.5, contrast=0.4, saturation=0.3, hue=0.3),\n",
    "    #v2.RandomHorizontalFlip(),\n",
    "    #v2.RandomVerticalFlip(),\n",
    "    #v2.RandomRotation(degrees=15, expand=True),\n",
    "    #v2.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    v2.Resize(size=(800, 1016), antialias=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train EfficientNet for image quality assessment (Task 1)\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "class Task1EfficientNetB0(nn.Module):\n",
    "    def __init__(self, learning_rate=1e-3):\n",
    "        super(Task1EfficientNetB0, self).__init__()\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Get model and replace the last layer\n",
    "        self.model = EfficientNet.from_pretrained('efficientnet-b0', num_classes=1)\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Freeze all layers except the last one\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze the last layer\n",
    "        for param in self.model._fc.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            pred = torch.sigmoid(self(x))\n",
    "        return pred\n",
    "\n",
    "\n",
    "model = Task1EfficientNetB0()\n",
    "\n",
    "model.to(device)\n",
    "print(\"Training model: \", model.__class__.__name__)\n",
    "\n",
    "metrics = [\n",
    "        Metric('auroc', roc_auc_score),\n",
    "        Metric('auprc', average_precision_score),\n",
    "        Metric('accuracy', lambda y_true, y_pred: (y_pred.round() == y_true).mean()),\n",
    "        Metric('sensitivity', sensitivity_score),\n",
    "        Metric('specificity', specificity_score)\n",
    "    ]\n",
    "\n",
    "class WandbLoggingHook(MetricCalculatedHook):\n",
    "        def on_metric_calculated(self, training_context: TrainingContext, metric: Metric, result, last_metric_for_epoch: bool):\n",
    "            import wandb\n",
    "            wandb.log(data={metric.name: result}, commit=last_metric_for_epoch)\n",
    "\n",
    "metrics_eval_strategy = DefaultMetricsEvaluationStrategy(metrics)\n",
    "if(use_wandb):\n",
    "    metrics_eval_strategy.register_metric_calculated_hook(WandbLoggingHook())\n",
    "\n",
    "config = {\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"dataset\": \"UWF4DR-Original\",\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 2,\n",
    "    \"model_type\": model.__class__.__name__\n",
    "}\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config[\"learning_rate\"])\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "train_data, val_data = dataset_builder.get_train_val()\n",
    "train_dataset = CustomDataset(train_data, transform=augment_for_task_1_training)\n",
    "val_dataset = CustomDataset(val_data, transform=augment_for_task_1_validation)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "trainer = Trainer(model, train_loader, val_loader, criterion, optimizer, lr_scheduler, device,\n",
    "                        metrics_eval_strategy=metrics_eval_strategy)\n",
    "\n",
    "if use_wandb:\n",
    "    wandb.init(entity='miccai-challenge-2024' ,project='task1', config=config)\n",
    "\n",
    "print(f\"Start training [{config['model_type']}] on [{config['dataset']}] dataset for [{config['epochs']}] epochs with batch size [{config['batch_size']}]\")\n",
    "\n",
    "trainer.train(num_epochs=config[\"epochs\"])\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
